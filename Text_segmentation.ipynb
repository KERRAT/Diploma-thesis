{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "image_folder = \"Ukr\"\n",
    "east_path = \"opencv_text_detection/frozen_east_text_detection.pb\"\n",
    "min_confidence = 0.5\n",
    "width = 320\n",
    "height = 320\n",
    "model_path = 'UkrainianOCR/examples/Ukrainian_OCR_extended_Resnet_with_blure_and_aug_new.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "\n",
    "output_folder = \"tmp\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, max_size=960):\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    if height > max_size or width > max_size:\n",
    "        if height > width:\n",
    "            new_height = max_size\n",
    "            new_width = int((width * max_size) / height)\n",
    "        else:\n",
    "            new_width = max_size\n",
    "            new_height = int((height * max_size) / width)\n",
    "        return cv2.resize(image, (new_width, new_height))\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_border(image):\n",
    "    top, bottom, left, right = 1, 1, 1, 1\n",
    "    image_without_borders = image[top:-bottom, left:-right]\n",
    "\n",
    "    image_with_border = cv2.copyMakeBorder(image_without_borders, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "    return image_with_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image(image):\n",
    "    if image is None or not isinstance(image, np.ndarray) or len(image.shape) < 2:\n",
    "        raise ValueError(\"Invalid input image.\")\n",
    "\n",
    "    if len(image.shape) == 2 or image.shape[2] == 1:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    elif image.shape[2] > 3:\n",
    "        image = image[:, :, :3]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def fully_clear_background(image):\n",
    "    formated = format_image(image)\n",
    "\n",
    "    image_with_border = clear_border(formated)\n",
    "\n",
    "    gray_image = cv2.cvtColor(image_with_border, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (3, 3), 0)\n",
    "\n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 27, 50)\n",
    "    \n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_line_empty(line, threshold=0.03, gray_threshold=130):\n",
    "    \"\"\"\n",
    "    Determine if a line contains text based on the number of non-white pixels.\n",
    "\n",
    "    Args:\n",
    "    line (numpy.ndarray): Image of the line.\n",
    "    threshold (float): Threshold for the proportion of non-white pixels to consider a line as empty. Default is 0.01 (1%).\n",
    "    gray_threshold (int): Gray level threshold to consider a pixel as non-white. Default is 200.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the line is empty, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    non_white_pixels = np.count_nonzero(line < gray_threshold)\n",
    "    total_pixels = line.size\n",
    "\n",
    "    if non_white_pixels / total_pixels < threshold:\n",
    "        print('true:', non_white_pixels / total_pixels)\n",
    "        return True\n",
    "    else:\n",
    "        print('false:', non_white_pixels / total_pixels)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import contours\n",
    "\n",
    "def segment_words(image, p_image, file_name, line_number):\n",
    "    converted = cv2.bitwise_not(p_image)\n",
    "    \n",
    "    blurred_image = cv2.GaussianBlur(converted, (5, 5), 0)\n",
    "\n",
    "    # Apply morphological dilation to connect words\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 12))\n",
    "    dilated = cv2.dilate(blurred_image, kernel, iterations=1)\n",
    "\n",
    "    cnts = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    \n",
    "\n",
    "    words_images = []\n",
    "    if len(cnts) > 0:\n",
    "        cnts, _ = contours.sort_contours(cnts, method=\"left-to-right\")\n",
    "        \n",
    "        for c in cnts:\n",
    "            area = cv2.contourArea(c)\n",
    "            if area > 10:\n",
    "                x, y, w, h = cv2.boundingRect(c)\n",
    "                ROI = image[y:y+h, x:x+w]\n",
    "                words_images.append(ROI)\n",
    "\n",
    "    return words_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_max_width = 32\n",
    "global_max_height = 32\n",
    "\n",
    "def extract_letters(word_image, global_max_width, global_max_height):\n",
    "    no_border = clear_border(word_image)\n",
    "    _, otsu_threshold = cv2.threshold(no_border, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 7))\n",
    "    dilated = cv2.dilate(otsu_threshold, kernel, iterations=1)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 4))\n",
    "    eroded = cv2.erode(dilated, kernel, iterations=1)\n",
    "\n",
    "    cnts, _ = cv2.findContours(eroded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    image_copy = word_image.copy()\n",
    "    image_copy = cv2.cvtColor(image_copy, cv2.COLOR_GRAY2BGR)\n",
    "    letters = []\n",
    "\n",
    "    cnts_sorted, _ = contours.sort_contours(cnts, method=\"left-to-right\")\n",
    "\n",
    "    # Process each contour and pad the images\n",
    "    for cont in cnts_sorted:\n",
    "        x, y, w, h = cv2.boundingRect(cont)\n",
    "        if h > 0 and w > 0:\n",
    "            cv2.rectangle(image_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            letter = word_image[y:y+h, x:x+w]\n",
    "            \n",
    "            thresh = cv2.adaptiveThreshold(letter, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 181, 40)\n",
    "\n",
    "            # Calculate padding for the current letter\n",
    "            pad_top = max(0, global_max_height - h)\n",
    "            pad_bottom = 2\n",
    "            pad_left = max(0, (global_max_width - w) // 2)\n",
    "            pad_right = max(0, global_max_width - w - pad_left)\n",
    "\n",
    "            # Pad the letter image to match the maximum dimensions\n",
    "            letter_padded = cv2.copyMakeBorder(thresh, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_CONSTANT, value=0)\n",
    "            letters.append(letter_padded)\n",
    "\n",
    "    return letters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def recognize_letter(letter_image, model):\n",
    "    letters = [\n",
    "    'А','Б','В','Г','Ґ','Д','Е','Є','Ж','З','И','І','Ї','Й','К',\n",
    "    'Л','М','Н','О','П','Р','С','Т','У','Ф','Х','Ц','Ч','Ш','Щ',\n",
    "    'Ь','Ю','Я','а','б','в','г','ґ','д','е','є','ж','з','и','і',\n",
    "    'ї','й','к','л','м','н','о','п','р','с','т','у','ф','х','ц',\n",
    "    'ч','ш','щ','ь','ю','я','1','2','3','4','5','6','7','8','9',\n",
    "    '0','№','%','@',',','.','?',':',';','\"','!','(',')','-','\\''\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Змінюємо розмір зображення літери до 32x32\n",
    "    resized_letter = cv2.resize(letter_image, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    blur_size = (3, 3)  # розмір ядра для гаусового блюру, можна змінювати за потреби\n",
    "    blur_sigma = 0  # відхилення, якщо дорівнює 0, відхилення обчислюється автоматично\n",
    "    letter_blured = cv2.GaussianBlur(resized_letter, blur_size, blur_sigma)\n",
    "\n",
    "    # Перевіряємо кількість каналів у зображенні\n",
    "    if len(letter_blured.shape) == 3:\n",
    "        # Конвертуємо зображення у відтінки сірого, якщо воно кольорове\n",
    "        gray_letter = cv2.cvtColor(letter_blured, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_letter = letter_blured\n",
    "    \n",
    "\n",
    "    # Конвертуємо в float32 та нормалізуємо\n",
    "    data = np.array(gray_letter, dtype=np.float32)\n",
    "    data = np.expand_dims(data, axis=-1)\n",
    "    data /= 255.0\n",
    "    \n",
    "    # Передбачаємо літеру за допомогою навченої моделі\n",
    "    prediction = model.predict(np.array([data]))[0]\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    probability = prediction[predicted_index]  # Отримуємо ймовірність передбаченого індексу\n",
    "    \n",
    "    predicted_letter = letters[predicted_index]  # Отримуємо передбачену літеру з масиву літер\n",
    "    \n",
    "    return predicted_letter, probability, predicted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_image(folder_path, file_name, image):\n",
    "    output_file = os.path.join(folder_path, file_name)\n",
    "    cv2.imwrite(output_file, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Ukr\\1_page.jpg\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[400,960,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m# Save the resized image, overwriting the original image\u001b[39;00m\n\u001b[0;32m     20\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(image_path, resized_image)\n\u001b[1;32m---> 22\u001b[0m lines \u001b[39m=\u001b[39m get_lines(image_path, kernel_size\u001b[39m=\u001b[39;49m\u001b[39m17\u001b[39;49m, sigma\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, theta\u001b[39m=\u001b[39;49m\u001b[39m9\u001b[39;49m, smooth_window_len\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, threshold\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m, peak_min_distance\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     24\u001b[0m \u001b[39m# Create a folder for the current image\u001b[39;00m\n\u001b[0;32m     25\u001b[0m current_image_folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_folder, file_name[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m])\n",
      "File \u001b[1;32md:\\programming\\Диплом\\Page_to_lines.py:155\u001b[0m, in \u001b[0;36mget_lines\u001b[1;34m(image_path, kernel_size, sigma, theta, smooth_window_len, threshold, peak_min_distance)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_lines\u001b[39m(image_path, kernel_size\u001b[39m=\u001b[39m\u001b[39m27\u001b[39m, sigma\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, theta\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, smooth_window_len\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, threshold\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, peak_min_distance\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m):\n\u001b[1;32m--> 155\u001b[0m   img_gray \u001b[39m=\u001b[39m read_and_prepare_image(image_path)\n\u001b[0;32m    156\u001b[0m   img_filtered \u001b[39m=\u001b[39m filter_image(img_gray, kernel_size, sigma, theta)\n\u001b[0;32m    157\u001b[0m   img_normalized \u001b[39m=\u001b[39m normalize_image(img_filtered)\n",
      "File \u001b[1;32md:\\programming\\Диплом\\Page_to_lines.py:26\u001b[0m, in \u001b[0;36mread_and_prepare_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m img \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mread_file(image_path)\n\u001b[0;32m     25\u001b[0m img \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mdecode_image(img, channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m img_gray \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mrgb_to_grayscale(img)\n\u001b[0;32m     27\u001b[0m img_gray \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(img_gray)\n\u001b[0;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m img_gray\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[400,960,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Mul]"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from opencv_text_detection.text_detection import text_detection\n",
    "\n",
    "\n",
    "# Assuming you have the other necessary functions defined above\n",
    "\n",
    "for file_name in os.listdir(image_folder):\n",
    "    if file_name.endswith(\".jpg\") or file_name.endswith(\".png\"):\n",
    "        image_path = os.path.join(image_folder, file_name)\n",
    "        print(f\"Processing {image_path}\")\n",
    "\n",
    "        if \"_page\" in file_name:\n",
    "            result = ''\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            # Resize the image before processing\n",
    "            resized_image = resize_image(image)\n",
    "\n",
    "            # Save the resized image, overwriting the original image\n",
    "            cv2.imwrite(image_path, resized_image)\n",
    "            \n",
    "            lines = get_lines(image_path, kernel_size=17, sigma=2, theta=9, smooth_window_len=4, threshold=0.3, peak_min_distance=2)\n",
    "\n",
    "            # Create a folder for the current image\n",
    "            current_image_folder = os.path.join(output_folder, file_name[:-4])\n",
    "            if not os.path.exists(current_image_folder):\n",
    "                os.makedirs(current_image_folder)\n",
    "\n",
    "            # Process each line and save it in the folder\n",
    "            for line_idx, line in enumerate(lines):\n",
    "                if not line.size == 0:  # Check if the line is not empty\n",
    "                    p_line = fully_clear_background(line)\n",
    "                    if not is_line_empty(p_line):  # Check if the line contains text\n",
    "                        words_images = segment_words(line, p_line, file_name, line_idx)\n",
    "                        \n",
    "                        # Create a folder for the current line\n",
    "                        current_line_folder = os.path.join(current_image_folder, f\"line_{line_idx}\")\n",
    "                        if not os.path.exists(current_line_folder):\n",
    "                            os.makedirs(current_line_folder)\n",
    "\n",
    "                        for word_idx, word_image in enumerate(words_images):\n",
    "\n",
    "                            letters = extract_letters(word_image,global_max_width, global_max_height)\n",
    "\n",
    "                            # Create a folder for the current word\n",
    "                            current_word_folder = os.path.join(current_line_folder, f\"word_{word_idx}\")\n",
    "                            if not os.path.exists(current_word_folder):\n",
    "                                os.makedirs(current_word_folder)\n",
    "                                \n",
    "                            letters_folder = os.path.join(output_folder, f\"letters\")\n",
    "                            if not os.path.exists(letters_folder):\n",
    "                                os.makedirs(letters_folder)\n",
    "                            \n",
    "                            # Save each letter in the folder\n",
    "                            for letter_idx, letter_image in enumerate(letters):\n",
    "                                save_image(current_word_folder, f\"letter_{letter_idx}.jpg\", letter_image)\n",
    "                                \n",
    "                                # Recognize the letter using the trained model                               \n",
    "\n",
    "                                predicted_letter, probability, predicted_index = recognize_letter(letter_image, model)\n",
    "                                result += predicted_letter\n",
    "                                save_image(letters_folder, f\"file_{file_name}_line_{line_idx}_word_{word_idx}_letter_{letter_idx}_prediction_{predicted_index}.jpg\", letter_image)\n",
    "                                print(f\"Letter {letter_idx} is recognized as '{predicted_letter}' with probability {probability:.2f}\")\n",
    "                                \n",
    "                            # Add space after each word\n",
    "                            result += ' '\n",
    "                            \n",
    "                            # Save word image in the line folder\n",
    "                            save_image(current_line_folder, f\"word_{word_idx}.jpg\", word_image)\n",
    "\n",
    "                # Save line image in the image folder only if it's not empty\n",
    "                if line.size > 0:\n",
    "                    save_image(current_image_folder, f\"line_{line_idx}.jpg\", line)\n",
    "\n",
    "            # Add space after each line\n",
    "            result += ' '\n",
    "            \n",
    "            print(result)\n",
    "        else:\n",
    "            data, result_img = text_detection(image_path, east_path, min_confidence, width, height)\n",
    "            save_image(output_folder, file_name, result_img)\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Ukr\\1_page.jpg\n",
      "x1= 0 , x2= 0 , Diff=  0\n",
      "x1= 0 , x2= 10 , Diff=  10\n",
      "x1= 10 , x2= 57 , Diff=  47\n",
      "x1= 57 , x2= 97 , Diff=  40\n",
      "x1= 97 , x2= 137 , Diff=  40\n",
      "x1= 137 , x2= 176 , Diff=  39\n",
      "x1= 176 , x2= 216 , Diff=  40\n",
      "x1= 216 , x2= 256 , Diff=  40\n",
      "x1= 256 , x2= 295 , Diff=  39\n",
      "x1= 295 , x2= 335 , Diff=  40\n",
      "x1= 335 , x2= 392 , Diff=  57\n",
      "true: 0.0\n",
      "false: 0.13982712765957447\n",
      "1/1 [==============================] - 1s 626ms/step\n",
      "Letter 0 is recognized as 'А' with probability 0.94\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 1 is recognized as 'н' with probability 0.97\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 2 is recognized as 'т' with probability 0.98\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 3 is recognized as '.' with probability 0.99\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 4 is recognized as 'р' with probability 0.82\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 5 is recognized as 'а' with probability 1.00\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Letter 6 is recognized as 'л' with probability 0.93\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 7 is recognized as 'ь' with probability 0.97\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 8 is recognized as '.' with probability 0.99\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 0 is recognized as '.' with probability 0.99\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Letter 0 is recognized as 'е' with probability 0.95\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Letter 1 is recognized as 'ф' with probability 0.93\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 2 is recognized as 'е' with probability 0.99\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 3 is recognized as 'к' with probability 0.97\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Letter 4 is recognized as 'т' with probability 0.80\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 5 is recognized as 'и' with probability 0.90\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 6 is recognized as 'в' with probability 0.76\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 7 is recognized as 'н' with probability 0.97\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 8 is recognized as 'и' with probability 0.98\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 9 is recognized as 'й' with probability 1.00\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 0 is recognized as 'в' with probability 0.71\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Letter 0 is recognized as 'л' with probability 0.99\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 1 is recognized as 'і' with probability 0.95\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 2 is recognized as 'к' with probability 0.99\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 3 is recognized as 'У' with probability 0.55\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Letter 4 is recognized as 'в' with probability 0.84\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Letter 5 is recognized as 'а' with probability 1.00\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Letter 6 is recognized as 'н' with probability 0.85\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 7 is recognized as 'н' with probability 0.76\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 8 is recognized as 'ї' with probability 0.75\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 0 is recognized as 'г' with probability 0.40\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 1 is recognized as '0' with probability 0.50\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Letter 2 is recognized as 'С' with probability 0.97\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Letter 3 is recognized as 'т' with probability 0.90\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 4 is recognized as 'р' with probability 0.98\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 5 is recognized as 'и' with probability 0.99\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 6 is recognized as 'х' with probability 0.91\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 0 is recognized as 'і' with probability 1.00\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Letter 0 is recognized as 'х' with probability 0.96\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 1 is recognized as 'р' with probability 0.98\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Letter 2 is recognized as '0' with probability 0.47\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 3 is recognized as 'н' with probability 0.93\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 4 is recognized as 'ї' with probability 0.96\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Letter 5 is recognized as 'ч' with probability 0.89\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 6 is recognized as 'н' with probability 0.96\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 7 is recognized as 'и' with probability 0.94\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 8 is recognized as 'х' with probability 0.92\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 0 is recognized as 'г' with probability 0.95\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 1 is recognized as 'е' with probability 0.98\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 2 is recognized as 'п' with probability 0.99\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 3 is recognized as 'а' with probability 1.00\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Letter 4 is recognized as '-' with probability 0.99\n",
      "false: 0.1540625\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 0 is recognized as 'т' with probability 0.83\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 1 is recognized as 'и' with probability 0.97\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 2 is recognized as 'т' with probability 0.80\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 3 is recognized as 'в' with probability 0.87\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 0 is recognized as 'р' with probability 1.00\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 1 is recognized as 'ї' with probability 0.83\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Letter 2 is recognized as '.' with probability 0.99\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 3 is recognized as 'З' with probability 0.88\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 4 is recognized as '.' with probability 0.97\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 5 is recognized as 'н' with probability 0.98\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 6 is recognized as 'О' with probability 0.61\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 7 is recognized as 'г' with probability 0.89\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Letter 8 is recognized as 'о' with probability 0.46\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 0 is recognized as 'ґ' with probability 0.98\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 1 is recognized as 'е' with probability 0.75\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 2 is recognized as 'н' with probability 0.88\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 3 is recognized as 'е' with probability 0.98\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 4 is recognized as 'З' with probability 0.88\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 5 is recognized as 'У' with probability 0.82\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Letter 0 is recognized as '(' with probability 0.98\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 1 is recognized as 'а' with probability 1.00\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 2 is recognized as 'л' with probability 0.79\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 3 is recognized as 'к' with probability 0.94\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 4 is recognized as 'О' with probability 0.67\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Letter 5 is recognized as 'г' with probability 0.94\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 6 is recognized as 'О' with probability 0.69\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 7 is recognized as 'л' with probability 0.86\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 8 is recognized as 'ь' with probability 0.91\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 9 is recognized as 'н' with probability 0.97\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 10 is recognized as 'и' with probability 0.97\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 11 is recognized as 'х' with probability 0.91\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Letter 12 is recognized as ',' with probability 0.98\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Letter 0 is recognized as 'Т' with probability 0.52\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Letter 1 is recognized as 'О' with probability 0.79\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 2 is recognized as 'к' with probability 0.92\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 3 is recognized as 'С' with probability 0.58\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 4 is recognized as 'и' with probability 0.99\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Letter 5 is recognized as 'ч' with probability 0.54\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Letter 6 is recognized as 'н' with probability 0.82\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Letter 7 is recognized as 'и' with probability 0.92\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 8 is recognized as 'х' with probability 0.90\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 9 is recognized as ',' with probability 0.88\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 0 is recognized as 'в' with probability 0.80\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 1 is recognized as 'і' with probability 0.98\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Letter 2 is recognized as 'р' with probability 1.00\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 3 is recognized as 'У' with probability 0.84\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 4 is recognized as 'С' with probability 0.63\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 5 is recognized as 'н' with probability 0.74\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Letter 6 is recognized as 'и' with probability 0.89\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 7 is recognized as 'х' with probability 0.95\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Letter 8 is recognized as ')' with probability 0.99\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 9 is recognized as ',' with probability 0.98\n",
      "false: 0.16130208333333335\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Letter 0 is recognized as 'ж' with probability 0.88\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 1 is recognized as 'и' with probability 0.95\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 2 is recognized as 'р' with probability 0.60\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Letter 3 is recognized as 'О' with probability 0.86\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 4 is recognized as 'В' with probability 0.57\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 5 is recognized as 'О' with probability 0.59\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 6 is recognized as 'і' with probability 0.84\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Letter 7 is recognized as '.' with probability 1.00\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 0 is recognized as 'д' with probability 0.99\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Letter 1 is recognized as 'и' with probability 0.92\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 2 is recognized as 'С' with probability 0.98\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 3 is recognized as 'т' with probability 0.80\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 4 is recognized as 'р' with probability 0.88\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 5 is recognized as 'О' with probability 0.78\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 6 is recognized as 'ф' with probability 0.97\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 7 is recognized as 'ї' with probability 0.99\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 8 is recognized as 'ї' with probability 1.00\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 0 is recognized as 'т' with probability 0.84\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 1 is recognized as 'з' with probability 0.36\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 0 is recognized as 'ц' with probability 0.77\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 1 is recognized as 'и' with probability 0.97\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 2 is recognized as 'р' with probability 0.64\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 3 is recognized as 'О' with probability 0.71\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 4 is recognized as 'З' with probability 0.95\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 5 is recognized as 'і' with probability 0.98\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 6 is recognized as 'В' with probability 0.56\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 0 is recognized as 'п' with probability 0.97\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 1 is recognized as 'е' with probability 0.98\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 2 is recognized as 'Ч' with probability 0.62\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 3 is recognized as 'і' with probability 1.00\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 4 is recognized as 'н' with probability 0.97\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 5 is recognized as 'к' with probability 0.98\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 6 is recognized as 'и' with probability 0.98\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 7 is recognized as ',' with probability 0.98\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 0 is recognized as 'д' with probability 0.81\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 1 is recognized as 'л' with probability 0.92\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 2 is recognized as 'я' with probability 0.33\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 0 is recognized as 'п' with probability 0.95\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 1 is recognized as 'р' with probability 0.76\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 2 is recognized as 'О' with probability 0.54\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 3 is recognized as 'ф' with probability 0.99\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Letter 4 is recognized as 'і' with probability 0.99\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 5 is recognized as 'л' with probability 0.63\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 6 is recognized as 'а' with probability 0.92\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 7 is recognized as 'к' with probability 0.92\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 8 is recognized as 'т' with probability 0.75\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 9 is recognized as 'и' with probability 0.97\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Letter 10 is recognized as 'к' with probability 0.98\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Letter 11 is recognized as '.' with probability 1.00\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 12 is recognized as 'и' with probability 0.87\n",
      "false: 0.16885683760683762\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 0 is recognized as 'З' with probability 0.82\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 1 is recognized as 'а' with probability 1.00\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Letter 2 is recognized as 'х' with probability 0.87\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 3 is recognized as 'в' with probability 0.65\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 4 is recognized as 'О' with probability 0.75\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 5 is recognized as 'р' with probability 0.86\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 6 is recognized as 'ю' with probability 0.68\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 7 is recognized as 'В' with probability 0.73\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 8 is recognized as 'а' with probability 1.00\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 9 is recognized as 'н' with probability 0.60\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 10 is recognized as 'ь' with probability 0.93\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 0 is recognized as 'п' with probability 0.91\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 1 is recognized as 'е' with probability 0.97\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 2 is recognized as 'ч' with probability 0.97\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Letter 3 is recognized as 'ї' with probability 0.74\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 4 is recognized as 'н' with probability 0.84\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 5 is recognized as 'к' with probability 0.93\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 6 is recognized as 'и' with probability 0.88\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 0 is recognized as 'В' with probability 0.85\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 1 is recognized as 'н' with probability 0.81\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 2 is recognized as 'а' with probability 0.94\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 3 is recognized as 'С' with probability 0.94\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 4 is recognized as 'л' with probability 0.61\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 5 is recognized as 'і' with probability 0.58\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 6 is recognized as 'д' with probability 0.99\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 7 is recognized as 'О' with probability 0.65\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 8 is recognized as 'к' with probability 0.63\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 0 is recognized as 'ш' with probability 0.99\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 1 is recognized as 'к' with probability 0.94\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 2 is recognized as 'і' with probability 0.73\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 3 is recognized as 'д' with probability 0.94\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 4 is recognized as 'л' with probability 0.97\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 5 is recognized as 'и' with probability 0.98\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 6 is recognized as 'в' with probability 0.80\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 7 is recognized as 'О' with probability 0.40\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Letter 8 is recognized as 'г' with probability 0.79\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Letter 9 is recognized as 'О' with probability 0.71\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 0 is recognized as 'в' with probability 0.61\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 1 is recognized as 'п' with probability 0.94\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Letter 2 is recognized as 'л' with probability 0.71\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Letter 3 is recognized as 'и' with probability 0.93\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 4 is recognized as 'в' with probability 0.76\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 5 is recognized as 'у' with probability 0.55\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 0 is recognized as 'р' with probability 0.99\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 1 is recognized as 'і' with probability 0.97\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 2 is recognized as 'З' with probability 0.96\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Letter 3 is recognized as 'н' with probability 0.88\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Letter 4 is recognized as 'и' with probability 0.96\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Letter 5 is recognized as 'х' with probability 0.80\n",
      "false: 0.16307291666666668\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Letter 0 is recognized as 'т' with probability 0.79\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 1 is recognized as 'О' with probability 0.68\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Letter 2 is recognized as 'к' with probability 0.95\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m         image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(image_folder, file_name)\n\u001b[0;32m     55\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing \u001b[39m\u001b[39m{\u001b[39;00mimage_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m         result \u001b[39m=\u001b[39m process_image(image_path, model)\n\u001b[0;32m     58\u001b[0m         \u001b[39mprint\u001b[39m(result)\n\u001b[0;32m     60\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mProcessing completed.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 46\u001b[0m, in \u001b[0;36mprocess_image\u001b[1;34m(image_path, model)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m# The temporary file will be deleted when the context manager exits\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m line_idx, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(lines):\n\u001b[1;32m---> 46\u001b[0m     result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m process_line(line, image_path, line_idx, model)\n\u001b[0;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "Cell \u001b[1;32mIn[27], line 23\u001b[0m, in \u001b[0;36mprocess_line\u001b[1;34m(line, file_name, line_idx, model)\u001b[0m\n\u001b[0;32m     21\u001b[0m         words_images \u001b[39m=\u001b[39m segment_words(line, p_line, file_name, line_idx)\n\u001b[0;32m     22\u001b[0m         \u001b[39mfor\u001b[39;00m word_idx, word_image \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words_images):\n\u001b[1;32m---> 23\u001b[0m             result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m process_word(word_image, file_name, line_idx, word_idx, model)\n\u001b[0;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m result \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[27], line 13\u001b[0m, in \u001b[0;36mprocess_word\u001b[1;34m(word_image, file_name, line_idx, word_idx, model)\u001b[0m\n\u001b[0;32m     11\u001b[0m letters \u001b[39m=\u001b[39m extract_letters(word_image, global_max_width, global_max_height)\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m letter_idx, letter_image \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(letters):\n\u001b[1;32m---> 13\u001b[0m     result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m process_letter(letter_image, file_name, line_idx, word_idx, letter_idx, model)\n\u001b[0;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m result \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m, in \u001b[0;36mprocess_letter\u001b[1;34m(letter_image, file_name, line_idx, word_idx, letter_idx, model)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_letter\u001b[39m(letter_image, file_name, line_idx, word_idx, letter_idx, model):\n\u001b[1;32m----> 5\u001b[0m     predicted_letter, probability, predicted_index \u001b[39m=\u001b[39m recognize_letter(letter_image, model)\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLetter \u001b[39m\u001b[39m{\u001b[39;00mletter_idx\u001b[39m}\u001b[39;00m\u001b[39m is recognized as \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpredicted_letter\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m with probability \u001b[39m\u001b[39m{\u001b[39;00mprobability\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m predicted_letter\n",
      "Cell \u001b[1;32mIn[25], line 33\u001b[0m, in \u001b[0;36mrecognize_letter\u001b[1;34m(letter_image, model)\u001b[0m\n\u001b[0;32m     30\u001b[0m data \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39m255.0\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[39m# Передбачаємо літеру за допомогою навченої моделі\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49marray([data]))[\u001b[39m0\u001b[39m]\n\u001b[0;32m     34\u001b[0m predicted_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(prediction)\n\u001b[0;32m     35\u001b[0m probability \u001b[39m=\u001b[39m prediction[predicted_index]  \u001b[39m# Отримуємо ймовірність передбаченого індексу\u001b[39;00m\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py:2220\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2211\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2212\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2213\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2214\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2217\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2218\u001b[0m         )\n\u001b[1;32m-> 2220\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   2221\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   2222\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2223\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   2224\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2225\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2226\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2227\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2228\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2229\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2230\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[0;32m   2231\u001b[0m )\n\u001b[0;32m   2233\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\data_adapter.py:1582\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1581\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1582\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\data_adapter.py:1262\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1261\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1263\u001b[0m     x,\n\u001b[0;32m   1264\u001b[0m     y,\n\u001b[0;32m   1265\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1266\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1267\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1268\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1269\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1270\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1271\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1272\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1273\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1274\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1275\u001b[0m )\n\u001b[0;32m   1277\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\data_adapter.py:349\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    347\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 349\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    353\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mshuffle_batch\u001b[39m(\u001b[39m*\u001b[39mbatch):\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\data_adapter.py:390\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    386\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m    387\u001b[0m         \u001b[39mlambda\u001b[39;00m d: tf\u001b[39m.\u001b[39mgather(d, i, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), data\n\u001b[0;32m    388\u001b[0m     )\n\u001b[1;32m--> 390\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(grab_batch, num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE)\n\u001b[0;32m    392\u001b[0m \u001b[39m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[39m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[0;32m    394\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2204\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2202\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39m, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   2203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2204\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[0;32m   2205\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2206\u001b[0m       map_func,\n\u001b[0;32m   2207\u001b[0m       num_parallel_calls,\n\u001b[0;32m   2208\u001b[0m       deterministic,\n\u001b[0;32m   2209\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2210\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5441\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[0;32m   5440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m-> 5441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[0;32m   5442\u001b[0m     map_func,\n\u001b[0;32m   5443\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[0;32m   5444\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[0;32m   5445\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[0;32m   5446\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5447\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    265\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[0;32m    272\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2610\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2602\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2603\u001b[0m \n\u001b[0;32m   2604\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2608\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2610\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[0;32m   2611\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2612\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2613\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2576\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2574\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 2576\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   2577\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m   2578\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   2579\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2760\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2758\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[0;32m   2759\u001b[0m   args, kwargs \u001b[39m=\u001b[39m placeholder_dict[\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m-> 2760\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   2762\u001b[0m graph_capture_container \u001b[39m=\u001b[39m graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2669\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   2666\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2667\u001b[0m ]\n\u001b[0;32m   2668\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m-> 2669\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[0;32m   2670\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   2671\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   2672\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   2673\u001b[0m         args,\n\u001b[0;32m   2674\u001b[0m         kwargs,\n\u001b[0;32m   2675\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   2676\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   2677\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   2678\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   2679\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   2680\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function_attributes,\n\u001b[0;32m   2681\u001b[0m     spec\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_spec,\n\u001b[0;32m   2682\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;49;00m\n\u001b[0;32m   2683\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;49;00m\n\u001b[0;32m   2684\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;49;00m\n\u001b[0;32m   2685\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;49;00m\n\u001b[0;32m   2686\u001b[0m     shared_func_graph\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m   2687\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1494\u001b[0m, in \u001b[0;36mConcreteFunction.__init__\u001b[1;34m(self, func_graph, attrs, shared_func_graph, spec)\u001b[0m\n\u001b[0;32m   1488\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_garbage_collector \u001b[39m=\u001b[39m ConcreteFunctionGarbageCollector(func_graph)\n\u001b[0;32m   1490\u001b[0m \u001b[39m# Pairs of forward and backward functions used for computing gradients.\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \u001b[39m# These each get a reference to the FuncGraph deleter since they use the\u001b[39;00m\n\u001b[0;32m   1493\u001b[0m \u001b[39m# FuncGraph directly.\u001b[39;00m\n\u001b[1;32m-> 1494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_delayed_rewrite_functions \u001b[39m=\u001b[39m _DelayedRewriteGradientFunctions(\n\u001b[0;32m   1495\u001b[0m     func_graph, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attrs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_garbage_collector)\n\u001b[0;32m   1496\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_first_order_tape_functions \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1497\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_higher_order_tape_functions \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:584\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions.__init__\u001b[1;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_function_pairs \u001b[39m=\u001b[39m {}\n\u001b[0;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph \u001b[39m=\u001b[39m func_graph\n\u001b[1;32m--> 584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_function \u001b[39m=\u001b[39m _EagerDefinedFunction(\n\u001b[0;32m    585\u001b[0m     _inference_name(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph\u001b[39m.\u001b[39;49mname), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph,\n\u001b[0;32m    586\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph\u001b[39m.\u001b[39;49minputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph\u001b[39m.\u001b[39;49moutputs, attrs)\n\u001b[0;32m    587\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs \u001b[39m=\u001b[39m attrs\n\u001b[0;32m    588\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:360\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.__init__\u001b[1;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[0;32m    358\u001b[0m   output_names \u001b[39m=\u001b[39m []\n\u001b[0;32m    359\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39m_c_graph\u001b[39m.\u001b[39mget() \u001b[39mas\u001b[39;00m c_graph:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m   fn \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_GraphToFunction_wrapper(\n\u001b[0;32m    361\u001b[0m       c_graph,\n\u001b[0;32m    362\u001b[0m       compat\u001b[39m.\u001b[39;49mas_str(name),\n\u001b[0;32m    363\u001b[0m       \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    364\u001b[0m       [o\u001b[39m.\u001b[39;49m_c_op \u001b[39mfor\u001b[39;49;00m o \u001b[39min\u001b[39;49;00m operations],  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    365\u001b[0m       [t\u001b[39m.\u001b[39;49m_as_tf_output() \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m inputs],  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    366\u001b[0m       [t\u001b[39m.\u001b[39;49m_as_tf_output() \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m outputs],  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    367\u001b[0m       output_names,\n\u001b[0;32m    368\u001b[0m       [o\u001b[39m.\u001b[39;49m_c_op \u001b[39mfor\u001b[39;49;00m o \u001b[39min\u001b[39;49;00m graph\u001b[39m.\u001b[39;49mcontrol_outputs],  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    369\u001b[0m       [],  \u001b[39m# control_output_names\u001b[39;49;00m\n\u001b[0;32m    370\u001b[0m       \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    371\u001b[0m       compat\u001b[39m.\u001b[39;49mas_str(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_func \u001b[39m=\u001b[39m c_api_util\u001b[39m.\u001b[39mScopedTFFunction(fn, name)\n\u001b[0;32m    375\u001b[0m \u001b[39mfor\u001b[39;00m name, attr_value \u001b[39min\u001b[39;00m attrs\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Page_to_lines import get_lines, display_lines\n",
    "import tempfile\n",
    "\n",
    "def process_letter(letter_image, file_name, line_idx, word_idx, letter_idx, model):\n",
    "    predicted_letter, probability, predicted_index = recognize_letter(letter_image, model)\n",
    "    print(f\"Letter {letter_idx} is recognized as '{predicted_letter}' with probability {probability:.2f}\")\n",
    "    return predicted_letter\n",
    "\n",
    "def process_word(word_image, file_name, line_idx, word_idx, model):\n",
    "    result = ''\n",
    "    letters = extract_letters(word_image, global_max_width, global_max_height)\n",
    "    for letter_idx, letter_image in enumerate(letters):\n",
    "        result += process_letter(letter_image, file_name, line_idx, word_idx, letter_idx, model)\n",
    "    return result + ' '\n",
    "\n",
    "def process_line(line, file_name, line_idx, model):\n",
    "    result = ''\n",
    "    if not line.size == 0:\n",
    "        p_line = fully_clear_background(line)\n",
    "        if not is_line_empty(p_line):\n",
    "            words_images = segment_words(line, p_line, file_name, line_idx)\n",
    "            for word_idx, word_image in enumerate(words_images):\n",
    "                result += process_word(word_image, file_name, line_idx, word_idx, model)\n",
    "    return result + ' '\n",
    "\n",
    "def process_image(image_path, model):\n",
    "    result = ''\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Resize the image before processing\n",
    "    resized_image = resize_image(image)\n",
    "\n",
    "    # Save the resized image in a temporary file\n",
    "    tmp_file_descriptor, tmp_file_name = tempfile.mkstemp(suffix='.jpg')\n",
    "    os.close(tmp_file_descriptor)\n",
    "    cv2.imwrite(tmp_file_name, resized_image)\n",
    "\n",
    "    lines = get_lines(tmp_file_name, kernel_size=17, sigma=2, theta=9, smooth_window_len=4, threshold=0.3, peak_min_distance=2)\n",
    "    \n",
    "    os.remove(tmp_file_name)\n",
    "\n",
    "    # The temporary file will be deleted when the context manager exits\n",
    "\n",
    "    for line_idx, line in enumerate(lines):\n",
    "        result += process_line(line, image_path, line_idx, model)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Assuming you have the other necessary functions defined above\n",
    "\n",
    "for file_name in os.listdir(image_folder):\n",
    "    if file_name.endswith(\".jpg\") or file_name.endswith(\".png\"):\n",
    "        image_path = os.path.join(image_folder, file_name)\n",
    "        print(f\"Processing {image_path}\")\n",
    "\n",
    "        result = process_image(image_path, model)\n",
    "        print(result)\n",
    "\n",
    "print(\"Processing completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
