{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "image_folder = \"Ukr\"\n",
    "model_path = 'C:/Models/Ukrainian_OCR_tf_2.1.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "\n",
    "output_folder = \"tmp\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def show_image(image, title=None, conversion=cv2.COLOR_BGR2RGB, dpi=10):\n",
    "\n",
    "    # Create a new figure with the specified dpi\n",
    "    plt.figure(dpi=dpi)\n",
    "\n",
    "    # Converts from one colour space to the other. this is needed as RGB\n",
    "    # is not the default colour space for OpenCV\n",
    "    image = cv2.cvtColor(image, conversion)\n",
    "\n",
    "    # Show the image\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # remove the axis / ticks for a clean looking image\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    # if a title is provided, show it\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, max_size=960):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    if height > max_size or width > max_size:\n",
    "        if height > width:\n",
    "            new_height = max_size\n",
    "            new_width = int((width * max_size) / height)\n",
    "        else:\n",
    "            new_width = max_size\n",
    "            new_height = int((height * max_size) / width)\n",
    "        return cv2.resize(image, (new_width, new_height))\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_border(image):\n",
    "    top, bottom, left, right = 1, 1, 1, 1\n",
    "    image_without_borders = image[top:-bottom, left:-right]\n",
    "\n",
    "    image_with_border = cv2.copyMakeBorder(image_without_borders, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "    return image_with_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_image(image):\n",
    "    if image is None or not isinstance(image, np.ndarray) or len(image.shape) < 2:\n",
    "        raise ValueError(\"Invalid input image.\")\n",
    "\n",
    "    if len(image.shape) == 2 or image.shape[2] == 1:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    elif image.shape[2] > 3:\n",
    "        image = image[:, :, :3]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_background(image):\n",
    "    formated = format_image(image)\n",
    "\n",
    "    image_with_border = clear_border(formated)\n",
    "\n",
    "    gray_image = cv2.cvtColor(image_with_border, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (3, 3), 0)\n",
    "\n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 27, 50)\n",
    "\n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_line_empty(line, threshold=0.03, gray_threshold=130):\n",
    "    \"\"\"\n",
    "    Determine if a line contains text based on the number of non-white pixels.\n",
    "\n",
    "    Args:\n",
    "    line (numpy.ndarray): Image of the line.\n",
    "    threshold (float): Threshold for the proportion of non-white pixels to consider a line as empty. Default is 0.01 (1%).\n",
    "    gray_threshold (int): Gray level threshold to consider a pixel as non-white. Default is 200.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the line is empty, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    non_white_pixels = np.count_nonzero(line < gray_threshold)\n",
    "    total_pixels = line.size\n",
    "\n",
    "    if non_white_pixels / total_pixels < threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_brightness_contrast(input_img, brightness = 0, contrast = 0):\n",
    "    \n",
    "    if brightness != 0:\n",
    "        if brightness > 0:\n",
    "            shadow = brightness\n",
    "            highlight = 255\n",
    "        else:\n",
    "            shadow = 0\n",
    "            highlight = 255 + brightness\n",
    "        alpha_b = (highlight - shadow)/255\n",
    "        gamma_b = shadow\n",
    "        \n",
    "        buf = cv2.addWeighted(input_img, alpha_b, input_img, 0, gamma_b)\n",
    "    else:\n",
    "        buf = input_img.copy()\n",
    "    \n",
    "    if contrast != 0:\n",
    "        f = 131*(contrast + 127)/(127*(131-contrast))\n",
    "        alpha_c = f\n",
    "        gamma_c = 127*(1-f)\n",
    "        \n",
    "        buf = cv2.addWeighted(buf, alpha_c, buf, 0, gamma_c)\n",
    "\n",
    "    return buf\n",
    "\n",
    "im = cv2.imread('Ukr/Franko_page.jpg')\n",
    "im = apply_brightness_contrast(im, contrast=40, brightness=20)\n",
    "show_image(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import contours\n",
    "\n",
    "def segment_words(image, p_image, file_name, line_number):\n",
    "    converted = cv2.bitwise_not(p_image)\n",
    "\n",
    "    blurred_image = cv2.GaussianBlur(converted, (5, 5), 0)\n",
    "\n",
    "    # Apply morphological dilation to connect words\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 12))\n",
    "    dilated = cv2.dilate(blurred_image, kernel, iterations=1)\n",
    "\n",
    "    cnts = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "\n",
    "    words_images = []\n",
    "    if len(cnts) > 0:\n",
    "        cnts, _ = contours.sort_contours(cnts, method=\"left-to-right\")\n",
    "\n",
    "        for c in cnts:\n",
    "            area = cv2.contourArea(c)\n",
    "            if area > 10:\n",
    "                x, y, w, h = cv2.boundingRect(c)\n",
    "                ROI = image[y:y+h, x:x+w]\n",
    "                words_images.append(ROI)\n",
    "\n",
    "    return words_images\n",
    "\n",
    "\n",
    "\n",
    "im = cv2.imread('tmp\\\\photo_2023-_page\\\\line_27.jpg')\n",
    "p_im = clear_background(im)\n",
    "im = segment_words(im, p_im, '', '')\n",
    "\n",
    "for word in im:\n",
    "    show_image(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABQCAYAAAC+neOMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAACv0lEQVR4nO3cv0p6YRzH8c9JTzqEBKU4ODoptCh4BXkVbc3egWubt6AuZXUBXUAQ5Ba0GFotIQQa0eLf9GmI4udPs09wTo/B57WV+fjtnT3DOfg4xhgD+daa7QEWubm5sT3CnCAApFIpdLtd27OspO3tbTw9Pb2H6na76HQ6tmdaaSv5r7eKFIqkUCSFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSApFUiiSQpEUiqRQJIUiKRRJoUgKRVIokkKRFIqkUCSFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSEE/F4/H48hkMgiHw56vfXd3h+vra0ynU8/XXsTXUDs7OyiVSohGo56vXS6X0Wg0MBwOPV97EV9Dua6Lzc1NbG1tebquMQaJRAKpVArPz89ot9sYj8eevsb//uQe5TgOdnd3UalUUCwWEYvFfH9NX99Ry4xGIwwGAyz7cJfrugiHw1hbm/97RiIRhEIhPD4+wnVdP0cFYDHUxcUFDg8Pl+4x2WwW+/v7iEQiM983xuD8/By1Wg3tdvtXPvBkLdTt7S1OT0/R6/W+/Jler4e9vb2FjzWbTZycnPzaZv4n9ygbFIqkUCSFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSApFshYqEAhgfX0dweD8JbFlj9libZJcLoeDgwO0Wi0cHR19njbkOA7y+Tzy+TySySQ2NjZsjTjDWqh0Oo10Oo16vY6zs7OZULlcDoVCYeG1clt8DfXw8IDj4+O5a97/ur+/x8vLy+fXxhhcXV2hWq3CcZwvn3d5eYnJZOLpvMs4xhgTi8V8OWjrY69Z9gtPp1MMh8OZuzGu6357Z+X19RWj0cizWb/ycfPW13fUZDJBv9//8fPG47HvNzR/anU2gRWnUCSFIikUSaFICkVSKJJCkYLA+3m4stjHWcGOjunmvAHz2MGQcgoS5wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABQCAYAAAC+neOMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAADeklEQVR4nO3cvUvrbBjH8V9sidbaDrV3fMUhWHzDQSr+G06K/4OTg5OISyf/ChXcijo5OXVzaHEoqDhoBkONqeArbcTcz+Bz9BRbzjU9Vw7P9YEsyXLxNb0by00MrbWG+KMu7gHaubi44B7hhzgATE9Pw/d97lkiKZvNol6vf4byfR/39/fcM0VaJD96USShiCQUkYQiklBEEopIQhFJKCIJRSShiCQUkYQiklBEEopIQhFJKCIJRSShiCQUkYQiklBEEopIQhFJKCIJRSShiOLcA/wuFouhq+v7bxeGIT4+Phgn+haZUMlkEktLS5iZmfk6V61WUSwW8fr6yjjZv7TWWimlAbAeSil9eHiowzD8Og4ODnQ2m2WfSymlI7VGGYbRcoyNjWF5eRmLi4vo7+/nHS5Kd9TR0ZH+XRAE+vn5WZfLZZ3P51nvqMisUe28vLzAcRxcX1+j0WiwzhLpUOVyGVtbW6jVanBdl3WWyIQKwxBPT08tWyRd18Xl5SXq9TrjZJ8MrbW2LIt9a6Jpmpibm8PQ0NDXOdd1cXZ2hiAI2OZSSgGI0B0VBAFOT0+5x+goUo8HUSahiNg/eplMBvPz80ilUj+u3d7eolKpsK5Rv7CHsm0bhUIBtm3/uHZ8fIy1tbVIfOuxh3p/f4fneUilUhgeHkY6nf661tfXB8MwGKf7xr5GXV1dYX19Haurq6hUKtzjdMR+R729veH8/By+7+Px8ZF7nI7Y76i/hYQiklBEEopIQhFJKCL2x4Oenh5YloXBwUEkk0nucTpiDzU+Po7NzU3Ytt3235ioYA/V29uLiYkJ5HI5mKbJPU5H7GvUzc0NCoUCNjY2UK1WucfpiD2U53koFovY39+H4zjc43TEHupvIaGIJBSRhCJifzwwTROZTAYDAwNIJBIt1xKJBEZGRhCPx/Hw8MD72zn3Jo2pqSm9u7urS6WS9jyvZZOG53m6VCrpnZ0dPTk5+f/epJFOp5HP55HL5RCPt46jlIJSCpZltfyWzoE9lOM42N7exujoKFZWVjA7O8s9UlvsoWq1Gvb29qCUwsLCgoT6k0ajgZOTk7Yv/Lq7u2PfRBKZ3SwA0N3djVgs9uO81hrNZhNhGP7nM0VuNwsANJtN7hE6kgdOIglFJKGIJBSRhCKSUEQSikhCEcWBz/fhivZ+vSvYkNd00/wDttv+MEszJ4EAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABQCAYAAAC+neOMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAAE3klEQVR4nO2cPUjzXBiG76Q/1tqqSCNqQXQSRZRWUAdFKIIgit0VBwURXFycBJ0dXVxcFBQdFEcnwUUEhwpCtaLtUDVUWytCW9M26fkGsSj+vKevb9L241yQpcnJubly8jQnacMRQggYf4TPd4Cv8Pl8+Y7wCT0AtLS0IBKJ5DtLQWKz2fD4+PgqKhKJIBwO5ztTQVOQp14hwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRwkRRote6w7KyMphMppzaKIqCWCwGWZZhsVhQUlICSZIQj8dVSvkZTUUZDAaMjY1hYGAgp3aiKGJ5eRnBYBDj4+Po7+/H/v4+1tbWkE6nVUr7EU1F6XQ6tLe3w+12g+M46naXl5fY2NhAKBSC0+mE2+2GKIowGAyQZRla/AJc81Pvb7DZbJicnEQ4HIbD4QAAdHZ2Yn5+Hn6/H3t7e3h6elI3BCGECIJAAKi+mEwmsrKyQjKZDMmFTCZDFEUhiqJk2759dnh4SBobG1XLLAgCEQSBaDqiFEXB6ekpdnd30djYiPb2duj1rxEkSYLH44Eoitnt7XY7nE4nSkpKPpyqhBAEAgGcnZ3B6/VqU9S1HFEAiNlsJlVVVWR2dpbE4/HsqAmHw2R0dJRUVVVll7GxMRIOh78cYaurq6S2tpZYrVbC8/z/a0QBQCKRQCKRQDweByEEiUQCd3d3CIVCEEUR0Wg0u20sFvu2UEuShGg0imQyqUnuvBfz6+trLCwswO/34+bmJt9xviXvouLxOM7Pz3F1dZXvKD/CpjCUMFGUaH5l7nK54HA40NnZCYPBoGX3v0Lzud7IyAimpqbA8zx4vngGtObFnOd56PX6nOZ6hUDxHNI8w0RRwkRRokmN4jgORqMRZrM5+02XTqeRTqchSRIymYwWMX6FJqJsNhsmJibQ1NSE7u5ucByH4+NjbG9v4/b2tij++K2JqPLycgwNDaGnpyf7mc/nw/r6OhKJhBYRfo2qohoaGuByuVBfX4+6uroP61pbWzE9PY1UKvVt+7a2NpSWlqoZkRpVRTU3N2NxcRE1NTXZG3RvdHV1oaOj48f2b9dchYCqKXieh9FohNFo/LROp9NBp9P99b7tdjv6+vqyIzKVSsHn8324n/UvKYzDlSMcx6G/v//DiHx4eMDc3BwODw9V6VMTUbIs4/Hx8cd6RAPHcaisrITFYoHVaoXVas2uMxgMOT9YzQVNRN3f32NpaQler/dX+zGZTJiZmcHg4OA/SkaPJqJeXl7g8XhwdHT0q/2YzWYMDw8jmUxmaxwhBLIsI51Oq/ogtKhqVCqVws7ODi4uLtDb24uRkREEg0Fsbm4iGAyq+t6pohIlyzIODg5wcHAARVEwNDQEURSxvr6OQCCgat+qirq5ucHW1haSySTu7+/V7Ep1VBXl9XoxPz8PAJo9f1MLVUUpioKXlxdV9h0KhXBycoKLiwtIkqRKH+/hCCGkurq6KGbw76moqIAgCJAkCaFQCLIsq9KPIAgAiqyYv+f5+RnPz8+a9cfucFLCRFHCRFHCRFHCRFHCRFGiB16fkjC+5u1dwRx7TTcd/wF//HvRy/UOQAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABQCAYAAAC+neOMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAADBklEQVR4nO3cPUvrYBjG8etJI75Qi9LkQSRacXBwcHF00MEv4N5v4GdwcioUnP0cbuIgTh0cLAoKzlK1SVFBsUVzn0EUSqPnPnCekx64ftClSZqbP31N4TEiIqDf8vIeIMvV1VXeIwzwAWB5eRlxHOc9y1AKggBJknyEiuMY7XY775mG2lC+9IYRQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQyn5Lh98ZmYGq6urGBsbc3aOOI5xenqK5+dnZ+cAHIdaWVlBvV5HGIbOztFoNLC9vf1/hxoZGcHU1BTK5fLAtk6ng9vbW6Rp+u3xpVIJs7Oz8P3BMZMkwd3dHZIkwfv7+1+dO4vTUD85Pj5GvV7H6+vrt/tsbGxgZ2cH09PTffeLCI6OjrC3t4dOp4P7+3vX4+YXKo5jNJtNvLy8fLtPpVLB29tb5rZ2u42zszN0u11XI/bhp54SQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlFJuoYrFIqIogrV24ApmqVRCFEUol8vwvOwRi8Ui5ubmYK1FoVBwPm9uF+7W19exv7+P8/Nz1Go13NzcAACMMdja2kK1WoW1FpOTk5nHb25uYmFhAc1mE7VaDa1Wy+m8TkOlaYper5d5FTIIAgRBAM/zMD4+/nW/MQbz8/NYW1uD53kQEfR6Pfi+//XsMsYgiiJEUYRCodB3vCtOQ11eXmJ3dxcTExPf7tNqtfqWZBIRHB4e4uHhAcYYAIC1FtVqFZVKxeW4PxMRCcNQAAztbWlpSRqNhmQ5OTmRxcVFZ+cOw1DCMJTc3qP+xOPjIw4ODnBxcTGw7fr6Gk9PT85nMCIi1tqhXmjLGIPR0dHMT8A0TdHtduFqqb7PP2//i2eUiPz4/9+/wC+cSgylxFBKDKXEUEoMpcRQSgyl5AMfv+Qp2+dawYbLdOv8ArVnKW4EifA1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABQCAYAAAC+neOMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAAE9klEQVR4nO2cu08ySxyG3wWEqCiuwiIBE5DCoLHwEi+VdlobOxspbWxsjFKYz05tNLGwsDNe/gMLjQU2xtiQaPASC9EoqBBvCMtlTkEkx7N8fqPnDMtJ5km22fnBvjzMDMNAViCEEHD+iEbtAIUIBoNqR1CgA4Dm5mY8PDyonaUkMZvNeHx8zIl6eHjA/f292plKmpIceqUIF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0UJF0WJjuWT2+129Pb2ory8XNF2fn6Ow8NDZDIZAIDBYEBvby8aGhoUtc/Pz9jf38fj4yPLuF9DCCEWi4UA+M+PgYEBcnFxQZ6fnxXH0tISKS8vz9eazWaysbFRsPbo6Ih0dHQwyfinw2KxEIvFQpj2KJ1Oh8rKSlRVVSnaHA4H2tvbkUgkAACiKMJmsxWsraurQ0tLy6dzkUgE19fXKNY/K5mK+or+/n54PJ78C9VqtbDZbAVr6+vrMTMzk5cKAOvr65ibm4Msy0XJy1SULMuIRqPQ6/WoqqpCWVlZvk0URYiiiFQqhdfXV2QyGSSTSSSTyXyNXq+H0WiEwWCAy+XKnyeEwGq1QhAElvE/wVRUIBDAxMQEnE4nxsfH4fF4FDWXl5dYXFzE7e2toq27uxtjY2MwmUwsY1LBVFQ4HMb29jbcbjdGRkYK1kSjUezs7OD8/FzRRgiB1+uF0WiERqP51IMEQYBWq4UgCEWZp0p6HRUMBjE/P4/l5WXc3Nx8auvq6sL09DS8Xi9EUWSeRbXJnIbT01OcnZ2hqakJPT09cDgcAHK9qbOzEx0dHfD7/djb20MsFmOapaRFAbnhl81mFcNLEAQIgqAYkqwo6aFXSnBRlHBRlKg2R8myjEQigbe3N2SzWbViUKOaqP39faytreHm5gaRSEStGNSoJuri4gJbW1uIx+NqRfgWTEW53W4MDg6ioaEhvwb6v8JUVFNTEyYnJ1FfXw+tVsvyUsxhKkoQBOh0Ouh0P7uMJElobW2Fy+VCTU1N/jwhBFdXVwgGgwgEAkUZviW9Mm9vb8fCwgIkSVLsIOzu7uLXr194eXnB09MT8yyqiTKZTHC73YjFYgiHw0ilUvm22tpaiKIIl8sFm82G2tpaxePj8Tju7u4+7V+xRNUdTqfTiUAggNnZWYRCIQC54To0NITR0VGYzeaCW8NqoJooq9UKq9UKILfb+fELi0ajQWNjI3p6ehQfAIQQyLKMdDpdtC3gD1Sfo5xOJ3w+X36eEQQBbW1t0GiU365kWcbm5ib8fj9OTk6QTqeLllM1UR/bJpIkYXh4+MuaD1KpFPx+P1ZXV5nn+yeqiTo+Pobf7/9Wr0gmkzg5OWGY6veoJurg4ABTU1PfXgMVc7j9HdVEZTIZyLJc9En5p/D9KEpUE2U0GuFwOCBJ0o+/4hQT1UT19fVhZWUFPp8vv54qZVR7K+12O+x2OwwGAyoqKtSKQQ2foyjhoihhOvRCoRA2NjZQXV3925rLy8uibJP8WwRCCJEkicmNtrRaLfR6/Ze/5GazWSSTyaL9Iey7WCwWAIx7VCaTwfv7O8tLFA0+R1HCRVHCRVHCRVHCRVHCRVHCRVHCRVGiA3L3w+UU5uNewQK/TTcdfwERdu8Tq19f6AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABQCAYAAAC+neOMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAAEZklEQVR4nO2czS8zXRiHf1pa4uMpWk20RQgSkfhK6I4NkbASS0sLW4ltLcTS0p9gI0HCykZIWIiF+IpOomiLhNEPIqo11fMuRN94Wtzv4p3TyH0ls5k5J/nlmntOTu5MTp4QQoD5EYPsANlQFEV2hAzyAaClpQWhUEh2lpzEarUiHA6/iwqFQri/v5edKafJyU8vF2FRRFgUERZFhEURYVFEWBQRFkWERRFhUURYFBEWRYRFEWFRRFgUERZFhEURYVFEWBQRFkWERRFhUURYFBEWRYRFEWFRRFgUERZFhEURYVFEWBQRFkWERRFhUURYFJF82QEAwGDI/r6EEMiVv7uli3I6nRgZGUFlZWXGM6/Xi7W1NcRiMQnJ/kIIIWw2mwAg5XK73UJRFPH29pZxLS8vC6vVKi0bAGGz2YTNZhO6VpTBYEBbWxvq6+vT95qamlBaWpr183M4HBgeHsbT0xMAIJlM4vDwEH6/X6/I/6JnRZnNZjE3NydCoVD6enh4EMlkUmQjkUiISCSSHhsMBsXY2NjvraiCggI4nU6Ul5fD6XSioqICeXl5AIBYLIaLiwskEomMeWVlZaiurkZ+/nvMwsJCmM1mPSJnoIsou90Oj8eDjo4OOByOtCQA8Pl8mJ6eRjAYzJjX19cHj8eD8vJyPWJ+iy6izGYzGhsb0d7enr73+vqKeDwOVVVxcnKC8/PzjHm1tbVIJpN6RPwRaduDnZ0dLCws4ObmBqqqyopBRpoon8+HxcXFb/dIHxvOj02nzM2n9A3nd/h8PszPz6O4uBgAoGkajo6OpGTJaVGnp6dQFOXT4v/29iYlizRRLpcLAwMDiMfjX45RVRXHx8fQNE3HZNmRJqq3txcdHR3frjvr6+uYmppCJBLRMVl2pInSNA3Pz88wmUyw2+0wmUwZYywWy5edBb2RlmJrawvj4+OYmZnB7e2trBhkpFXU3d0d9vb2EIvFEI1Gs7ZZEokE96M+8Pv9mJ2dxZ8/fzKeBQIBPD8/S0iViXRRqqpiaWlJdowfkbZGtba2YmJiAqOjo1mrKdeQVlE9PT3o6urC7u4uDg4O8Pj4KCsKCWmijEYjjEYjrFYr3G437HY7FEVBOBxGXV0d6urq0mNjsRi8Xm+60ykFPTqcDQ0NYnt7O2sXMx6Pi5ubG7G/vy/6+/uFwWAQk5OT4vLyUgQCAREIBMTm5qZoa2v7/T3zZDIJVVVxfX0Ni8WCkpKS9DOz2Yzq6mqYTCa4XC64XC7U1NSgpqYmvdl8eXmR1tlMo0dFFRUVic7OTjE4OChWV1dFKpXK2h8/PDwUGxsb4uLi4tMYRVFEd3f376+ol5cX7O/vo7CwEENDQ3h9fc06rrm5Gc3NzQDwaYymaUilUnpE/RJdF3NN07CysoKzs7P/NC8ajeLq6up/SkUjTwghqqqq+FikL7DZbAD4Jw0yLIoIiyLCooiwKCIsigiLIsKiiLAoIiyKCIsikg+8n4fLZOfjrOA8Pqabxj/dp+mM8jQSnQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABQCAYAAAC+neOMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAAB60lEQVR4nO3cMYrCQACF4TfR2KUxiV0s0ggWojfRi2jtWQS9iBfwBmITbDUJiBYqSraQwIYV9nUzxftO8PiZSboxVVVVkH95tgd8s9/vbU/4ow0Aw+EQeZ7b3uKkKIpQFMUnVJ7nOJ/Ptjc5zcmr5yKFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSApFUiiSQpEUiqRQJIUiKRRJoUgKRVIokkKRFIqkUCSFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSApFUiiSQpEUiqRQJIUiKRRJoUgKRVIokkKRFIqkUCSFIikUSaFICkVSKJJCkRSK1LY9oOb7PgaDAaIoQpZlOB6Ptic1OHOigiDAYrHAer3GdDqFMcb2pAZnQrVaLcRxjH6/jyRJkCQJwjCE57kx0Y0Vv3ieh9lshs1mg/l8jiAIbE8C4NA3qmaMQZqmSNMUl8sFvu/bngTAwRPlKoUiOXP17vc7ttstiqLAZDLBeDy2PanBmVDX6xWr1QqdTgfL5RKj0cj2pAZnQgHA4/HA8/lElmXY7XY4HA54vV62ZwEATFVVVa/Xc+rptm63izAMcbvdcDqd8H6/rW2J4xiAYyeqVpYlyrK0PaNBfz2SQpEUiqRQJIUiKRSpDXzew5Xv6reCjZ7p5vwAxalj3Vlb48IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_size = 32\n",
    "\n",
    "\n",
    "def extract_letters(word_image, img_size):\n",
    "    no_border = clear_border(word_image)\n",
    "    _, otsu_threshold = cv2.threshold(no_border, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 3))\n",
    "    dilated = cv2.dilate(otsu_threshold, kernel, iterations=1)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    eroded = cv2.erode(dilated, kernel, iterations=1)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 5))\n",
    "    dilated = cv2.dilate(eroded, kernel, iterations=1)\n",
    "\n",
    "    cnts, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    image_copy = word_image.copy()\n",
    "    image_copy = cv2.cvtColor(image_copy, cv2.COLOR_GRAY2BGR)\n",
    "    letters = []\n",
    "\n",
    "    cnts_sorted, _ = contours.sort_contours(cnts, method=\"left-to-right\")\n",
    "\n",
    "    # Process each contour, resize or pad the images\n",
    "    for cont in cnts_sorted:\n",
    "        x, y, w, h = cv2.boundingRect(cont)\n",
    "        if h > 0 and w > 0:\n",
    "            cv2.rectangle(image_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            letter = word_image[y:y+h, x:x+w]\n",
    "\n",
    "            thresh = cv2.adaptiveThreshold(letter, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 181, 40)\n",
    "\n",
    "            if h > img_size or w > img_size:\n",
    "                # Resize the letter image while keeping the aspect ratio\n",
    "                aspect_ratio = min(float(w), float(h)) / max(float(w), float(h))\n",
    "\n",
    "                if h > img_size:\n",
    "                    h = img_size\n",
    "                    w = int(aspect_ratio * h)\n",
    "                else: # w > img_size\n",
    "                    w = img_size\n",
    "                    h = int(aspect_ratio * w)\n",
    "\n",
    "                letter_processed = cv2.resize(thresh, (w, h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Calculate padding for the current letter\n",
    "            pad_top = img_size - h\n",
    "            pad_bottom = 2\n",
    "            pad_left = (img_size - w) // 2\n",
    "            pad_right = img_size - w - pad_left\n",
    "\n",
    "\n",
    "            # Pad the letter image to match the maximum dimensions\n",
    "            letter_processed = cv2.copyMakeBorder(thresh, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_CONSTANT, value=0)\n",
    "            letters.append(letter_processed)\n",
    "\n",
    "    return letters\n",
    "\n",
    "\n",
    "\n",
    "im = cv2.imread('tmp\\\\photo_2023-_page\\\\line_27\\\\word_6.jpg')\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "letters = extract_letters(im, 32)\n",
    "\n",
    "for letter in letters:\n",
    "    show_image(letter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABPCAYAAABMHVPCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAACtElEQVR4nO3czUoyUQDG8WfKMRcigSktWrZScGPgFehVtGvtHbRt1y1UG8u8AC9AEHQntBFM3YQQZEgbazTntIji9c3sCWYahee382OOx79yFjPMsYwxBvKjEACkUikMh8Og57KSdnZ28Pj4+B5qOBzi4eEh6DmttI2gJ7AuFIqkUCSFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSApFUiiSQpEUiqRQJIUiKRRJoUgKRVIokkKRFIqkUCSFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSCE/B9/d3UU2m0UkEvF87F6vh5ubG7iu6/nYi/gaKpPJ4PT0FIlEwvOxz87O0G634TiO52Mv4mso27axvb2NeDzu6bjGGOzt7SGVSmE0GmEwGGA6nXr6Gf9byzXKsizk83mcn5/j+PgYyWTS98/09R+1zGQywcvLC5bds2TbNiKRCDY2vv6esVgMW1tbuL+/h23bfk4VQICh6vU6SqXS0jXm4OAAR0dHiMVic88bY1Cr1XB1dYXBYPAnNzwFFqrb7aJSqWA8Hn/7nvF4jMPDw4WvdTodXF9f/9livpZrVBAUiqRQJIUiKRRJoUgKRVIokkKRFIqkUCSFIikUSaFICkUKLNTm5ibC4TBCoa+nxJa9FpTAZpLL5XBycoLb21tcXl5+7jZkWRYKhQIKhQL29/cRjUaDmuKcwEKl02mk02k0m01Uq9W5ULlcDsViceG58qD4Guru7g7lcvnLOe9/9ft9PD09fT42xqDVauHi4gKWZX17XKPRwGw283S+y1jGGJNMJn3ZaOtjrVn2hV3XheM4c1djbNv+8crK6+srJpOJZ3P9zsfFW1//UbPZDM/Pz78+bjqd+n5B87dWZxFYcQpFUiiSQpEUiqRQJIUiKRQpBLzvhyuLfewVbGn3ac4bCg+/Bocg2gwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter: п; probability: 0.9945397973060608\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABPCAYAAABMHVPCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAADbklEQVR4nO3cPUvrbhjH8V9sidbaDrV3fMQhWHzCQSq+DSfF9+Dk4CTi0slXoYJbUScnp24OLQ4FEQfNYKgxFXykjZj7DJ6jp9hyrul/Rf7XB7Iky8W38TYtNzG01hrin+IAMD09Dd/3uWeJpGw2i3q9/hHK933c3d1xzxRpXdwD/BQSikhCEUkoIglFJKGIJBSRhCKSUEQSikhCEUkoIglFJKGIJBSRhCKSUEQSikhCEUkoIglFJKGIJBSRhCKSUEQSiijOPcDfYrEYurq+PrswDPH+/s440ZfIhEomk1haWsLMzMznuWq1imKxiJeXF8bJftNaa6WUBsB6KKX04eGhDsPw8zg4ONDZbJZ9LqWUjtQaZRhGyzE2Nobl5WUsLi6iv7+fd7go3VFHR0f6b0EQ6KenJ10ul3U+n2e9oyKzRrXz/PwMx3FwdXWFRqPBOkukQ5XLZWxtbaFWq8F1XdZZIhMqDEM8Pj62bJF0XRcXFxeo1+uMk30wtNbasiz2rYmmaWJubg5DQ0Of51zXxdnZGYIgYJtLKQUgQndUEAQ4PT3lHqOjSD0eRJmEImL/08tkMpifn0cqlfp27ebmBpVKhXWN+oM9lG3bKBQKsG3727Xj42Osra1F4r8ee6i3tzd4nodUKoXh4WGk0+nPa319fTAMg3G6L+xr1OXlJdbX17G6uopKpcI9Tkfsd9Tr6yvOz8/h+z4eHh64x+mI/Y76KSQUkYQiklBEEopIQhGxPx709PTAsiwMDg4imUxyj9MRe6jx8XFsbm7Ctu22X2Oigj1Ub28vJiYmkMvlYJom9zgdsa9R19fXKBQK2NjYQLVa5R6nI/ZQnuehWCxif38fjuNwj9MRe6ifQkIRSSgiCUXE/nhgmiYymQwGBgaQSCRariUSCYyMjCAej+P+/p73t3PuTRpTU1N6d3dXl0ol7XleyyYNz/N0qVTSOzs7enJy8v+9SSOdTiOfzyOXyyEebx1HKQWlFCzLavktnQN7KMdxsL29jdHRUaysrGB2dpZ7pLbYQ9VqNezt7UEphYWFBQn1L41GAycnJ21f+HV7e8u+iSQyu1kAoLu7G7FY7Nt5rTWazSbCMPzPZ4rcbhYAaDab3CN0JA+cRBKKSEIRSSgiCUUkoYgkFJGEIooDH+/DFe39eVewIW+fpvkFzQP7ppvPvPkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter: і; probability: 0.9938076138496399\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABPCAYAAABMHVPCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAAE1UlEQVR4nO2cTUgqXRjH/zN+ZJYV4kQfELmKIgoNrEURSBBEYfuiRUEEbdq0Cmrdsk2bNgVFLYqWrQQ3EbQwCCyjdGE1aPZBkDrqjOddRFLcW/d4uzPmy/nBbJw553n4zZnHOXN0OEIIAeOP6AGgra0N9/f3xc7lR2Kz2fDw8PAq6v7+HvF4vNg5/Wj4YidQKjBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlDBRlOi1DlhRUQGTyVRQG0VR8PLyAlmWUVlZibKyMkiShEQioVKWv6KpKIPBgPHxcQwODhbUThRFrKysIBKJYGJiAgMDAzg4OMD6+jqy2axK2X5EU1E6nQ6dnZ0YHR0Fx3HU7S4uLrC5uYloNAqn04nR0VGIogiDwQBZlqHFD5s1v/T+BpvNhqmpKcTjcTgcDgCAy+XCwsICQqEQ9vf38fT0pG4ShBAiCAIBoPpmMpnI6uoqyeVypBByuRxRFIUoipJv+/aZz+cjdrtdtZwFQSCCIBBNR5SiKDg5OcHe3h7sdjs6Ozuh17+mIEkS/H4/RFHMH9/Y2Ain04mysrIPlyohBOFwGKenpwgEAtoUdS1HFABiNpuJ1Wolc3NzJJFI5EdNPB4nY2NjxGq15rfx8XESj8d/O8LW1tZIfX09sVgshOf5/9eIAoBkMolkMolEIgFCCJLJJG5vbxGNRiGKIh4fH/PHvry8fFqoJUnC4+Mj0um0JnkXvZhfXV1hcXERoVAI19fXxU7nU4ouKpFI4OzsDJeXl8VO5UvYFIYSJooSze/M3W43HA4HXC4XDAaDluG/heZzPY/Hg+npafA8D54vnQGteTHneR56vb6gud5PoHROaZFhoihhoijRpEZxHAej0Qiz2Zz/pstms8hms5AkCblcTos0voUmomw2GyYnJ9HS0oKenh5wHIejoyPs7Ozg5uamJP74rYmoqqoqDA8Po7e3N/9ZMBjExsYGksmkFil8G1VFNTc3w+12o6mpCQ0NDR/2tbe3Y2ZmBplM5tP2HR0dKC8vVzNFalQV1draiqWlJdTV1eUf0L3R3d2Nrq6uL9u/3XP9BFTNgud5GI1GGI3GX/bpdDrodLq/7ruxsRH9/f35EZnJZBAMBj88z/qX/IzTVSAcx2FgYODDiLy7u8P8/Dx8Pp8qMTURJcsyHh4evqxHNHAch5qaGlRWVsJiscBiseT3GQyGghdWC0ETUbFYDMvLywgEAt/qx2QyYXZ2FkNDQ/8oM3o0EZVKpeD3+3F4ePitfsxmM0ZGRpBOp/M1jhACWZaRzWZVXQgtqRqVyWSwu7uL8/Nz9PX1wePxIBKJYGtrC5FIBMFgULXYJSVKlmV4vV54vV4oioLh4WGIooiNjQ2Ew2FVY6sq6vr6Gtvb20in04jFYmqGUh1VRQUCASwsLACAZutvaqGqKEVRkEqlVOk7Go3i+PgY5+fnkCRJlRjv4QghpLa2tiRm8O+prq6GIAiQJAnRaBSyLKsSRxAEACVWzN/z/PyM5+dnzeKxJ5yUMFGUMFGUMFGUMFGUMFGU6IHXVRLG73l7VzDH3j5Nx3+WJHlH2OCN9gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter: д; probability: 0.9970826506614685\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABPCAYAAABMHVPCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAAC+klEQVR4nO3cv0rrYBzG8edNI/6hFqVJEIlWHBwcXBwddPAG3HsHXoOTU6Hg7HW4iYM4dXCwKDg4S9UmRQXFFs17BvFAaet5Dpz3tMLzgS5N0vz4UvongddYay3kj3wAWF1dRZIkw55lJAVBgDRNP0MlSYJmsznsmUaaN+wBfgqFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSApFUiiSQpEUiqRQJIUiKRRJoUgKRVIokkKRFIqkUCSFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSApFUiiS7/LF5+bmsL6+jomJCWfnSJIE5+fneHl5cXYOwHGotbU1VKtVhGHo7By1Wg27u7s/O9TY2BhmZmZQLBZ7trVaLdzd3SHLsoHHFwoFzM/Pw/d7x0zTFPf390jTFB8fH/907n6chvrO6ekpqtUq3t7eBu6ztbWFvb09zM7Odj1vrcXJyQkODg7QarXw8PDgetzhhUqSBPV6Ha+vrwP3KZVKeH9/77ut2Wzi4uIC7Xbb1Yhd9K1HUiiSQpEUiqRQJIUiKRRJoUgKRVIokkKRFIqkUCSFIikUaWih8vk84jhGFEU9VzALhQLiOEaxWITn9R8xn89jYWEBURQhl8s5n3doF+42NzdxeHiIy8tLVCoV3N7eAgCMMdjZ2UG5XEYURZienu57/Pb2NpaWllCv11GpVNBoNJzO6zRUlmXodDp9r0IGQYAgCOB5HiYnJ38/b4zB4uIiNjY24HkerLXodDrwff/3u8sYgziOEccxcrlc1/GuOA11fX2N/f19TE1NDdyn0Wh0LclkrcXx8TEeHx9hjAEARFGEcrmMUqnkctzvWWttGIYWwMg+VlZWbK1Ws/2cnZ3Z5eVlZ+cOw9CGYWiH9hn1N56ennB0dISrq6uebTc3N3h+fnY+g7HW2iiKRnqhLWMMxsfH+34DZlmGdrsNVyvQfd28/RHvKGvtt/f//gf94CQpFEmhSApFUiiSQpEUiqRQJB/4/Ccv/X2tFWy0+jTnF8uPJuQHwXj0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter: п; probability: 0.9904229640960693\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABPCAYAAABMHVPCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAAE6klEQVR4nO2cu08qTRyG3wWEoHhZhAUCJiAFUWPhJQqVdlobOxstbWxsjFIY7dRGEwsLO+PlP7DQWGBjjA2JBC+xEIniNd5lucxXEMnxW49n9PuG5STzJNvs/GBfHmaGYSArEEIIOH9EBwD19fW4vr5WO0tRYrFYcHNzkxN1fX2Nq6srtTMVNRq1A/wtcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGUcFGU6Fg+udPpRCAQgNFoVLQdHR1hd3cXmUwGAGAwGBAIBFBTU6OofXh4wPb2Nm5ubljG/RpCCLFarQTA/350dXWR4+Nj8vDwoDjm5uaI0WjM11osFrKysvJp7d7eHmlpaWGS8U+H1WolVquVMO1ROp0OZWVlKC8vV7S5XC40Nzfj7e0NACCKIhwOx6e11dXVaGho+HDu8vISZ2dnKNQfBpmK+orOzk7U1dXlX6hWq4XD4fi01m63Y3x8PC8VAJaXlzE1NQVZlguSl6koWZZxe3sLvV6P8vJylJSU5NtEUYQoikilUnh6ekImk0EymUQymczX6PV6mEwmGAwGeDye/HlCCGw2GwRBYBn/A0xFhcNhDA8Pw+12Y2hoCHV1dYqak5MTzM7O4vz8XNHW3t6OwcFBVFZWsoxJBVNRiUQC6+vr8Hq96Ovr+7Tm9vYWGxsbODo6UrQRQjAwMACTyQSNRvOhBwmCAK1WC0EQCjJPFfU6KhqNYnp6GvPz84jH4x/a2traMDY2hoGBAYiiyDyLapM5DQcHBzg8PITP54Pf74fL5QKQ602tra1oaWlBKBTC1tYW7u7umGYpalFAbvhls1nF8BIEAYIgKIYkK4p66BUTXBQlXBQlqs1Rsizj7e0Nz8/PyGazasWgRjVR29vbWFpaQjwex+XlpVoxqFFN1PHxMdbW1vDy8qJWhG/BVJTX60V3dzdqamrya6C/FaaifD4fRkZGYLfbodVqWV6KOUxFCYIAnU4Hne5nl5EkCY2NjfB4PKiqqsqfJ4Tg9PQU0WgU4XC4IMO3qFfmzc3NmJmZgSRJih2Ezc1NTExM4PHxEff398yzqCaqsrISXq8Xd3d3SCQSSKVS+Taz2QxRFOHxeOBwOGA2mxWPf3l5wcXFxYf9K5aousPpdrsRDocxOTmJWCwGIDdce3p60N/fD4vF8unWsBqoJspms8FmswHI7Xa+/8Ki0WhQW1sLv9+v+AAghECWZaTT6YJtAb+j+hzldrsRDAbz84wgCGhqaoJGo/x2JcsyVldXEQqFEIlEkE6nC5ZTNVHv2yaSJKG3t/fLmndSqRRCoRAWFxeZ5/s3qona399HKBT6Vq9IJpOIRCIMU/0e1UTt7OxgdHT022ugQg63X1FNVCaTgSzLBZ+Ufwrfj6JENVEmkwkulwuSJP34K04hUU1UR0cHFhYWEAwG8+upYka1t9LpdMLpdMJgMKC0tFStGNTwOYoSLooSpkMvFothZWUFFRUVv605OTkpyDbJf0UghBBJkpjcaEur1UKv13/5S242m0UymSzYH8K+i9VqBcC4R2UyGby+vrK8RMHgcxQlXBQlXBQlXBQlXBQlXBQlXBQlXBQlOiB3P1zO57zfK1jgd5+m4x8nnuyJ/k1mJQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter: р; probability: 0.9920141100883484\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter: и; probability: 0.9843477010726929\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABPCAYAAABMHVPCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAAEXElEQVR4nO2cu0s7SxiG3ySaRLz8Ek0MaBJFUUEEjYKm00YRtBJLSwtbwVYLsbT0T7ARVNDKRhS0EAvxhgkYL/ECusYbkuuumVOIOXgS9TvF2fHI98A2OzPw8uTLMHwsYxBCCDDfkgcAjY2NiEQisrP8SBwOB+7v799ERSIR3N3dyc70ozHKDvB/gUURYVFEWBQRFkWERRFhUURYFBEWRYRFEWFRRFgUERZFhEURYVFEWBQRFkWERRFhUURYFBEWRYRFEWFRRFgUERZFhEURYVFEWBQRFkWERRFhUURYFBEWRYRFEWFRRPJkBwAAozH37yWEwE/5aFm6KLfbjYGBAZSVlWWNBQIBLC8vIxaLSUj2D4QQwul0CgBSHr/fL4LBoHh9fc16FhYWhMPhkJYNgHA6ncLpdApdK8poNKK5uRk1NTWZd/X19SguLs7596usrER/fz9eXl4AAJqmYW9vD+fn53pF/hs9K8pisYjp6WkRiUQyz9PTk9A0TeQimUyKh4eHzNyLiwsxNDT0eysqPz8fbrcbdrsdbrcbpaWlMBgMAIBYLIbT01Mkk8msdSUlJaioqEBe3ltMq9UKi8WiR+QsdBHlcrkwPj4On8+HysrKjCQACIVCmJiYwMXFRda6rq4ujI+Pw2636xHzS3QRZbFYUFdXh5aWlsy7VCqFRCIBRVFweHiIk5OTrHVVVVXQNE2PiN8i7XiwubmJ2dlZXF9fQ1EUWTHISBMVCoUwNzf35Rnp/cD5fuiUefiUfuD8ilAohJmZGRQWFgIAVFXF/v6+lCw/WtTR0RGCweCHzf/19VVKFmmiPB4Penp6kEgkPp2jKAoODg6gqqqOyXIjTVRnZyd8Pt+X+87KygrGxsbw8PCgY7LcSBOlqiqi0SjMZjNcLhfMZnPWHJvN9mlnQW+kpVhfX8fw8DAmJydxc3MjKwYZaRV1e3uL7e1txGIxPD4+5myzJJNJ7ke9c35+jqmpKfz58ydrLBwOIxqNSkiVjXRRiqJgfn5edoxvkbZHNTU1YWRkBIODgzmr6achraI6OjrQ1taGra0t7O7u4vn5WVYUEtJEmUwmmEwmOBwO+P1+uFwuBINB3N/fo7q6GtXV1Zm5sVgMgUAg0+mUgh4dztraWrGxsZGzi5lIJMT19bXY2dkR3d3dwmg0itHRUXF2dibC4bAIh8NibW1NNDc3//6euaZpUBQFV1dXsNlsKCoqyoxZLBZUVFTAbDbD4/HA4/HA6/XC6/VmDpvxeFxaZzODHhVVUFAgWltbRW9vr1haWhLpdDpnf3xvb0+srq6K09PTD3OCwaBob2///RUVj8exs7MDq9WKvr4+pFKpnPMaGhrQ0NAAAB/mqKqKdDqtR9RP0XUzV1UVi4uLOD4+/lfrHh8fcXl5+R+lomEQQojy8nK+FukTnE4nAP5IgwyLIsKiiLAoIiyKCIsiwqKIsCgiLIoIiyLCoojkAW/34TK5eb8r2MC3T9P4C/PP5wK8XFH3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter: є; probability: 0.9629616141319275\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter: м; probability: 0.9823300838470459\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter: щ; probability: 0.5488680005073547\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAABPCAYAAABMHVPCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAGKAAABigEzlzBYAAAB4ElEQVR4nO3cMYrCQBjF8TfR2KUxiV0s0ggWojfRi2jtWQS9iBfwCjbBVpOAaKGizBZLYAML+7qZhfc7wePPRLvPWGst5E9dABiPxyjL0vUWLyVJgqqqvkOVZYnL5eJ6k9cC1wP+C4UiKRRJoUgKRVIokkKRFIqkUCSFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSApFUiiSQpEUiqRQJIUiKRRJoUgKRVIokkKRFIqkUCSFIikUSaFICkVSKJJCkRSKpFAkhSIpFEmhSApFUiiSQpEUiqRQJIUiKRRJoUgKRVIokkKRFIrUdT2gEYYhRqMRkiRBURQ4nU6uJ7V486KiKMJqtcJ2u8V8PocxxvWkFm9CdTodpGmK4XCILMuQZRniOEYQ+DHRjxU/BEGAxWKB3W6H5XKJKIpcTwLg0W9UwxiDPM+R5zmu1yvCMHQ9CYCHL8pXCkXy5tN7PB7Y7/eoqgqz2QzT6dT1pBZvQt1uN2w2G/R6PazXa0wmE9eTWrwJBQDP5xOv1wtFUeBwOOB4POL9frueBQAw1lo7GAy8Ot3W7/cRxzHu9zvO5zM+n4+zLWmaAvDsRTXqukZd165ntOhfj6RQJIUiKRRJoUgKReoC3/dw5XfNrWCj69OcL9vRYVO6tJ9jAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 220x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter: ,; probability: 0.6331358551979065\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def recognize_letter(letter_image, model, attempts=3, min_probability=0.2):\n",
    "    letters = [\n",
    "        'А','Б','В','Г','Ґ','Д','Е','Є','Ж','З','И','І','Ї','Й','К',\n",
    "        'Л','М','Н','О','П','Р','С','Т','У','Ф','Х','Ц','Ч','Ш','Щ',\n",
    "        'Ь','Ю','Я','а','б','в','г','ґ','д','е','є','ж','з','и','і',\n",
    "        'ї','й','к','л','м','н','о','п','р','с','т','у','ф','х','ц',\n",
    "        'ч','ш','щ','ь','ю','я','1','2','3','4','5','6','7','8','9',\n",
    "        '0','№','%','@',',','.','?',':',';','\"','!','(',')','-','\\''\n",
    "    ]\n",
    "\n",
    "    for attempt in range(attempts):\n",
    "        # Змінюємо розмір зображення літери до 32x32\n",
    "        resized_letter = cv2.resize(letter_image, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        blur_size = (3, 3)  # розмір ядра для гаусового блюру, можна змінювати за потреби\n",
    "        blur_sigma = 0  # відхилення, якщо дорівнює 0, відхилення обчислюється автоматично\n",
    "        letter_blured = cv2.GaussianBlur(resized_letter, blur_size, blur_sigma)\n",
    "\n",
    "        # Перевіряємо кількість каналів у зображенні\n",
    "        if len(letter_blured.shape) == 3:\n",
    "            # Конвертуємо зображення у відтінки сірого, якщо воно кольорове\n",
    "            gray_letter = cv2.cvtColor(letter_blured, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray_letter = letter_blured\n",
    "\n",
    "        # Конвертуємо в float32 та нормалізуємо\n",
    "        data = np.array(gray_letter, dtype=np.float32)\n",
    "        data = np.expand_dims(data, axis=-1)\n",
    "        data /= 255.0\n",
    "\n",
    "        # Передбачаємо літеру за допомогою навченої моделі\n",
    "        prediction = model.predict(np.array([data]))[0]\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        probability = prediction[predicted_index]  # Отримуємо ймовірність передбаченого індексу\n",
    "        predicted_letter = letters[predicted_index]  # Отримуємо передбачену літеру з масиву літер\n",
    "\n",
    "        if probability >= min_probability:\n",
    "            return predicted_letter, probability, predicted_index\n",
    "        else:\n",
    "            # Змінюємо зображення перед наступною спробою\n",
    "            if attempt == 0:\n",
    "                # Зменшення зображення\n",
    "                scale_percent = 80\n",
    "                width = int(letter_image.shape[1] * scale_percent / 100)\n",
    "                height = int(letter_image.shape[0] * scale_percent / 100)\n",
    "                dim = (width, height)\n",
    "                letter_image = cv2.resize(letter_image, dim, interpolation=cv2.INTER_AREA)\n",
    "            elif attempt == 1:\n",
    "                # Збільшення зображення\n",
    "                scale_percent = 120\n",
    "                width = int(letter_image.shape[1] * scale_percent / 100)\n",
    "                height = int(letter_image.shape[0] * scale_percent / 100)\n",
    "                dim = (width, height)\n",
    "                letter_image = cv2.resize(letter_image, dim, interpolation=cv2.INTER_AREA)\n",
    "            elif attempt == 2:\n",
    "                # Поворот на 180 градусів\n",
    "                letter_image = cv2.rotate(letter_image, cv2.ROTATE_180)\n",
    "        # Якщо тричі ймовірність залишається низькою, повертаємо \"_\"\n",
    "    return \"_\", 0, -1\n",
    "\n",
    "for letter in letters:\n",
    "    show_image(letter)\n",
    "    pre_letter, probability, predicted_index = recognize_letter(letter, model)\n",
    "    print(f'letter: {pre_letter}; probability: {probability}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_image(folder_path, file_name, image):\n",
    "    output_file = os.path.join(folder_path, file_name)\n",
    "    cv2.imwrite(output_file, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Ukr\\2_page.jpg\n",
      "x1= 0 , x2= 0 , Diff=  0\n",
      "x1= 0 , x2= 9 , Diff=  9\n",
      "x1= 9 , x2= 70 , Diff=  61\n",
      "x1= 70 , x2= 108 , Diff=  38\n",
      "x1= 108 , x2= 145 , Diff=  37\n",
      "x1= 145 , x2= 183 , Diff=  38\n",
      "x1= 183 , x2= 220 , Diff=  37\n",
      "x1= 220 , x2= 256 , Diff=  36\n",
      "x1= 256 , x2= 293 , Diff=  37\n",
      "x1= 293 , x2= 330 , Diff=  37\n",
      "x1= 330 , x2= 366 , Diff=  36\n",
      "x1= 366 , x2= 375 , Diff=  9\n",
      "  актраль еф етив ний в лікуванні готрих і хроніцних гепа-  титв рі3 н0ю №незу (алкогольних, токсичних, вірусних),  жиро в ої дитрофії ш цирозів печінки, для профілатик.и  захв о р юван ь печінки внасл;док шкімивого впливу різних  то ксинів ( алкоголь, жирна та го_ра їжа, напівфабрикати,  ко нсерванти), порушеннях режиму харчування (діта),  пр ий о мі лікарських засобів ( антибїотики, проґи3апальнї,  се рцев о-%динні, антитромботичні, протитуберкульо3ні,  -пф.о-т-№ ибкові засоби тощо), ммі0терапії, променевій   \n",
      "Processing Ukr\\Franko_page.jpg\n",
      "x1= 0 , x2= 0 , Diff=  0\n",
      "x1= 0 , x2= 9 , Diff=  9\n",
      "x1= 9 , x2= 21 , Diff=  12\n",
      "x1= 21 , x2= 51 , Diff=  30\n",
      "x1= 51 , x2= 79 , Diff=  28\n",
      "x1= 79 , x2= 107 , Diff=  28\n",
      "x1= 107 , x2= 134 , Diff=  27\n",
      "x1= 134 , x2= 163 , Diff=  29\n",
      "x1= 163 , x2= 195 , Diff=  32\n",
      "x1= 195 , x2= 236 , Diff=  41\n",
      "x1= 236 , x2= 264 , Diff=  28\n",
      "x1= 264 , x2= 292 , Diff=  28\n",
      "x1= 292 , x2= 319 , Diff=  27\n",
      "x1= 319 , x2= 347 , Diff=  28\n",
      "x1= 347 , x2= 376 , Diff=  29\n",
      "x1= 376 , x2= 403 , Diff=  27\n",
      "x1= 403 , x2= 431 , Diff=  28\n",
      "x1= 431 , x2= 459 , Diff=  28\n",
      "x1= 459 , x2= 498 , Diff=  39\n",
      "x1= 498 , x2= 532 , Diff=  34\n",
      "x1= 532 , x2= 559 , Diff=  27\n",
      "x1= 559 , x2= 587 , Diff=  28\n",
      "x1= 587 , x2= 615 , Diff=  28\n",
      "x1= 615 , x2= 642 , Diff=  27\n",
      "x1= 642 , x2= 671 , Diff=  29\n",
      "x1= 671 , x2= 722 , Diff=  51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m         image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(image_folder, file_name)\n\u001b[0;32m     54\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing \u001b[39m\u001b[39m{\u001b[39;00mimage_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m         result \u001b[39m=\u001b[39m process_image(image_path, model, \u001b[39m32\u001b[39;49m)\n\u001b[0;32m     57\u001b[0m         \u001b[39mprint\u001b[39m(result\u001b[39m.\u001b[39mlower())\n\u001b[0;32m     59\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mProcessing completed.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[55], line 45\u001b[0m, in \u001b[0;36mprocess_image\u001b[1;34m(image_path, model, img_size)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39m# The temporary file will be deleted when the context manager exits\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m line_idx, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(lines):\n\u001b[1;32m---> 45\u001b[0m     result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m process_line(line, image_path, line_idx, model, img_size)\n\u001b[0;32m     47\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mlower()\n",
      "Cell \u001b[1;32mIn[55], line 22\u001b[0m, in \u001b[0;36mprocess_line\u001b[1;34m(line, file_name, line_idx, model, img_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m         words_images \u001b[39m=\u001b[39m segment_words(line, p_line, file_name, line_idx)\n\u001b[0;32m     21\u001b[0m         \u001b[39mfor\u001b[39;00m word_idx, word_image \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words_images):\n\u001b[1;32m---> 22\u001b[0m             result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m process_word(word_image, file_name, line_idx, word_idx, model, img_size)\n\u001b[0;32m     23\u001b[0m \u001b[39mreturn\u001b[39;00m result \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[55], line 12\u001b[0m, in \u001b[0;36mprocess_word\u001b[1;34m(word_image, file_name, line_idx, word_idx, model, img_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m letters \u001b[39m=\u001b[39m extract_letters(word_image, img_size)\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m letter_idx, letter_image \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(letters):\n\u001b[1;32m---> 12\u001b[0m     result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m process_letter(letter_image, letter_idx, model)\n\u001b[0;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m result \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[55], line 5\u001b[0m, in \u001b[0;36mprocess_letter\u001b[1;34m(letter_image, letter_idx, model)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_letter\u001b[39m(letter_image, letter_idx, model):\n\u001b[1;32m----> 5\u001b[0m     predicted_letter, probability, predicted_index \u001b[39m=\u001b[39m recognize_letter(letter_image, model)\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m predicted_letter\n",
      "Cell \u001b[1;32mIn[53], line 32\u001b[0m, in \u001b[0;36mrecognize_letter\u001b[1;34m(letter_image, model, attempts, min_probability)\u001b[0m\n\u001b[0;32m     29\u001b[0m data \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39m255.0\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[39m# Передбачаємо літеру за допомогою навченої моделі\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49marray([data]))[\u001b[39m0\u001b[39m]\n\u001b[0;32m     33\u001b[0m predicted_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(prediction)\n\u001b[0;32m     34\u001b[0m probability \u001b[39m=\u001b[39m prediction[predicted_index]  \u001b[39m# Отримуємо ймовірність передбаченого індексу\u001b[39;00m\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:88\u001b[0m, in \u001b[0;36mdisable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_multi_worker_mode():  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m     86\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not supported in multi-worker mode.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     87\u001b[0m       method\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n\u001b[1;32m---> 88\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1240\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1237\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope():\n\u001b[0;32m   1239\u001b[0m   \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1240\u001b[0m   data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mDataHandler(\n\u001b[0;32m   1241\u001b[0m       x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1242\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1243\u001b[0m       steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   1244\u001b[0m       initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   1245\u001b[0m       epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1246\u001b[0m       max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1247\u001b[0m       workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1248\u001b[0m       use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1249\u001b[0m       model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n\u001b[0;32m   1251\u001b[0m   \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1100\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1101\u001b[0m     x,\n\u001b[0;32m   1102\u001b[0m     y,\n\u001b[0;32m   1103\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1104\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1105\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1106\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1107\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1108\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1109\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1110\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1111\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mds_context\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1112\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   1114\u001b[0m strategy \u001b[39m=\u001b[39m ds_context\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1115\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter\u001b[39m.\u001b[39mget_dataset()\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:362\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m     flat_dataset \u001b[39m=\u001b[39m flat_dataset\u001b[39m.\u001b[39mshuffle(\u001b[39m1024\u001b[39m)\u001b[39m.\u001b[39mrepeat(epochs)\n\u001b[0;32m    360\u001b[0m   \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m--> 362\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mflat_map(slice_batch_indices)\n\u001b[0;32m    364\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1652\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1630\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_map\u001b[39m(\u001b[39mself\u001b[39m, map_func):\n\u001b[0;32m   1631\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Maps `map_func` across this dataset and flattens the result.\u001b[39;00m\n\u001b[0;32m   1632\u001b[0m \n\u001b[0;32m   1633\u001b[0m \u001b[39m  Use `flat_map` if you want to make sure that the order of your dataset\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1650\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1652\u001b[0m   \u001b[39mreturn\u001b[39;00m FlatMapDataset(\u001b[39mself\u001b[39;49m, map_func)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4070\u001b[0m, in \u001b[0;36mFlatMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   4068\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m   4069\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[1;32m-> 4070\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m StructuredFunctionWrapper(\n\u001b[0;32m   4071\u001b[0m     map_func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(), dataset\u001b[39m=\u001b[39;49minput_dataset)\n\u001b[0;32m   4072\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure, DatasetSpec):\n\u001b[0;32m   4073\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   4074\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`map_func` must return a `Dataset` object. Got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   4075\u001b[0m           \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure)))\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3221\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3218\u001b[0m resource_tracker \u001b[39m=\u001b[39m tracking\u001b[39m.\u001b[39mResourceTracker()\n\u001b[0;32m   3219\u001b[0m \u001b[39mwith\u001b[39;00m tracking\u001b[39m.\u001b[39mresource_tracker_scope(resource_tracker):\n\u001b[0;32m   3220\u001b[0m   \u001b[39m# TODO(b/141462134): Switch to using garbage collection.\u001b[39;00m\n\u001b[1;32m-> 3221\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m wrapper_fn\u001b[39m.\u001b[39;49mget_concrete_function()\n\u001b[0;32m   3223\u001b[0m   \u001b[39mif\u001b[39;00m add_to_graph:\n\u001b[0;32m   3224\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function\u001b[39m.\u001b[39madd_to_graph(ops\u001b[39m.\u001b[39mget_default_graph())\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2531\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2524\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2525\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2526\u001b[0m \n\u001b[0;32m   2527\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   2528\u001b[0m \u001b[39m    *args: inputs to specialize on.\u001b[39;00m\n\u001b[0;32m   2529\u001b[0m \u001b[39m    **kwargs: inputs to specialize on.\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2531\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[0;32m   2532\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2533\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2534\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2495\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 2496\u001b[0m   graph_function, args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   2497\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature:\n\u001b[0;32m   2498\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2777\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2774\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_define_function_with_shape_relaxation(args, kwargs)\n\u001b[0;32m   2776\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mmissed\u001b[39m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 2777\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   2778\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mprimary[cache_key] \u001b[39m=\u001b[39m graph_function\n\u001b[0;32m   2779\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, args, kwargs\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2657\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2652\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   2653\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2654\u001b[0m ]\n\u001b[0;32m   2655\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   2656\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 2657\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   2658\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   2659\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   2660\u001b[0m         args,\n\u001b[0;32m   2661\u001b[0m         kwargs,\n\u001b[0;32m   2662\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   2663\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   2664\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   2665\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   2666\u001b[0m         override_flat_arg_shapes\u001b[39m=\u001b[39;49moverride_flat_arg_shapes,\n\u001b[0;32m   2667\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   2668\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   2669\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   2670\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   2671\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   2672\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   2673\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   2674\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:981\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    979\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m--> 981\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[0;32m    983\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m    985\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m    986\u001b[0m                                   expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3214\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3208\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[0;32m   3209\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[0;32m   3210\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[0;32m   3211\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   3212\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[0;32m   3213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m-> 3214\u001b[0m   ret \u001b[39m=\u001b[39m _wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   3215\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[0;32m   3216\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3156\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>._wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack_args(nested_args):\n\u001b[0;32m   3154\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[1;32m-> 3156\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[0;32m   3157\u001b[0m \u001b[39m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[39;00m\n\u001b[0;32m   3158\u001b[0m \u001b[39m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[39;00m\n\u001b[0;32m   3159\u001b[0m \u001b[39m# those tensors into a single tensor, because the customized\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3165\u001b[0m \u001b[39m# the return value into a single tensor can use an explicit\u001b[39;00m\n\u001b[0;32m   3166\u001b[0m \u001b[39m# `tf.stack()` before returning.\u001b[39;00m\n\u001b[0;32m   3167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:262\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 262\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    263\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    264\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:492\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    489\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    491\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_whitelisted(f):\n\u001b[1;32m--> 492\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    494\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:346\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    343\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    347\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:348\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__.<locals>.slice_batch_indices\u001b[1;34m(indices)\u001b[0m\n\u001b[0;32m    346\u001b[0m num_in_full_batch \u001b[39m=\u001b[39m num_full_batches \u001b[39m*\u001b[39m batch_size\n\u001b[0;32m    347\u001b[0m first_k_indices \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mslice(indices, [\u001b[39m0\u001b[39m], [num_in_full_batch])\n\u001b[1;32m--> 348\u001b[0m first_k_indices \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39;49mreshape(\n\u001b[0;32m    349\u001b[0m     first_k_indices, [num_full_batches, batch_size])\n\u001b[0;32m    351\u001b[0m flat_dataset \u001b[39m=\u001b[39m dataset_ops\u001b[39m.\u001b[39mDatasetV2\u001b[39m.\u001b[39mfrom_tensor_slices(first_k_indices)\n\u001b[0;32m    352\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_partial_batch_size:\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:193\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmanip.reshape\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(tensor, shape, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m  \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Reshapes a tensor.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39m  Given `tensor`, this operation returns a new `tf.Tensor` that has the same\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39m    A `Tensor`. Has the same type as `tensor`.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m   result \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49mreshape(tensor, shape, name)\n\u001b[0;32m    194\u001b[0m   tensor_util\u001b[39m.\u001b[39mmaybe_set_static_shape(result, shape)\n\u001b[0;32m    195\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8086\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   8084\u001b[0m     _ops\u001b[39m.\u001b[39mraise_from_not_ok_status(e, name)\n\u001b[0;32m   8085\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m-> 8086\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[0;32m   8087\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mReshape\u001b[39;49m\u001b[39m\"\u001b[39;49m, tensor\u001b[39m=\u001b[39;49mtensor, shape\u001b[39m=\u001b[39;49mshape, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   8088\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[0;32m   8089\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:742\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    737\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    738\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[0;32m    739\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    740\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    741\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 742\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    743\u001b[0m                              name\u001b[39m=\u001b[39;49mscope, input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m    744\u001b[0m                              attrs\u001b[39m=\u001b[39;49mattr_protos, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m    746\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[0;32m    750\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:593\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    591\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[0;32m    592\u001b[0m   inputs[i] \u001b[39m=\u001b[39m inp\n\u001b[1;32m--> 593\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    594\u001b[0m     op_type, inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    595\u001b[0m     compute_device)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3319\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3316\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3317\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3318\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3319\u001b[0m   ret \u001b[39m=\u001b[39m Operation(\n\u001b[0;32m   3320\u001b[0m       node_def,\n\u001b[0;32m   3321\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   3322\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m   3323\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   3324\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[0;32m   3325\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   3326\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[0;32m   3327\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m   3328\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m   3329\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1816\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1814\u001b[0m   \u001b[39mif\u001b[39;00m op_def \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1815\u001b[0m     op_def \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39m_get_op_def(node_def\u001b[39m.\u001b[39mop)\n\u001b[1;32m-> 1816\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_op \u001b[39m=\u001b[39m _create_c_op(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph, node_def, inputs,\n\u001b[0;32m   1817\u001b[0m                             control_input_ops, op_def)\n\u001b[0;32m   1818\u001b[0m   name \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_str(node_def\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1819\u001b[0m \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1654\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1650\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[39m.\u001b[39mas_str(name),\n\u001b[0;32m   1651\u001b[0m                                          serialized)\n\u001b[0;32m   1653\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1654\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_FinishOperation(op_desc)\n\u001b[0;32m   1655\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1656\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1657\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mstr\u001b[39m(e))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Page_to_lines import get_lines, display_lines\n",
    "import tempfile\n",
    "\n",
    "def process_letter(letter_image, letter_idx, model):\n",
    "    predicted_letter, probability, predicted_index = recognize_letter(letter_image, model)\n",
    "    return predicted_letter\n",
    "\n",
    "def process_word(word_image, file_name, line_idx, word_idx, model, img_size):\n",
    "    result = ''\n",
    "    letters = extract_letters(word_image, img_size)\n",
    "    for letter_idx, letter_image in enumerate(letters):\n",
    "        result += process_letter(letter_image, letter_idx, model)\n",
    "    return result + ' '\n",
    "\n",
    "def process_line(line, file_name, line_idx, model, img_size):\n",
    "    result = ''\n",
    "    if not line.size == 0:\n",
    "        p_line = clear_background(line)\n",
    "        if not is_line_empty(p_line):\n",
    "            words_images = segment_words(line, p_line, file_name, line_idx)\n",
    "            for word_idx, word_image in enumerate(words_images):\n",
    "                result += process_word(word_image, file_name, line_idx, word_idx, model, img_size)\n",
    "    return result + ' '\n",
    "\n",
    "def process_image(image_path, model, img_size):\n",
    "    result = ''\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Resize the image before processing\n",
    "    resized_image = resize_image(image)\n",
    "\n",
    "    # Save the resized image in a temporary file\n",
    "    tmp_file_descriptor, tmp_file_name = tempfile.mkstemp(suffix='.jpg')\n",
    "    os.close(tmp_file_descriptor)\n",
    "    cv2.imwrite(tmp_file_name, resized_image)\n",
    "\n",
    "    lines = get_lines(tmp_file_name, kernel_size=17, sigma=3, theta=9, smooth_window_len=3, threshold=0.3, peak_min_distance=2)\n",
    "\n",
    "    os.remove(tmp_file_name)\n",
    "\n",
    "    # The temporary file will be deleted when the context manager exits\n",
    "\n",
    "    for line_idx, line in enumerate(lines):\n",
    "        result += process_line(line, image_path, line_idx, model, img_size)\n",
    "\n",
    "    return result.lower()\n",
    "\n",
    "# Assuming you have the other necessary functions defined above\n",
    "\n",
    "for file_name in os.listdir(image_folder):\n",
    "    if file_name.endswith(\".jpg\") or file_name.endswith(\".png\"):\n",
    "        image_path = os.path.join(image_folder, file_name)\n",
    "        print(f\"Processing {image_path}\")\n",
    "\n",
    "        result = process_image(image_path, model, 32)\n",
    "        print(result.lower())\n",
    "\n",
    "print(\"Processing completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
